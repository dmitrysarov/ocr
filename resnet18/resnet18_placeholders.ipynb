{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                                          \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"                                       \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"  \n",
    "from tensorflow.contrib.slim.nets import resnet_v2, resnet_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers as layers_lib\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.nn import ctc_loss, conv2d\n",
    "import numpy as np\n",
    "resnet_v2_block = resnet_v2.resnet_v2_block\n",
    "resnet_v2 = resnet_v2.resnet_v2\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v2_26_base(inputs,\n",
    "                 num_classes=None,\n",
    "                 is_training=True, # True - due to update batchnorm layers\n",
    "                 global_pool=False,\n",
    "                 output_stride=1, # effective stride \n",
    "                 reuse=None,\n",
    "                 include_root_block=False, #first conv layer. Removed due to max pool supression. We need large receprive field\n",
    "                 scope='resnet_v2_26'):\n",
    "  \n",
    "    \"\"\"\n",
    "    Tensorflow resnet_v2 use only bottleneck blocks (consist of 3 layers).\n",
    "    Thus, this resnet layer model consist of 26 layers.\n",
    "    I put stride = 2 on each block due to increase receptive field.\n",
    "\n",
    "    \"\"\"\n",
    "    blocks = [\n",
    "      resnet_v2_block('block1', base_depth=64, num_units=2, stride=2),\n",
    "      resnet_v2_block('block2', base_depth=128, num_units=2, stride=2),\n",
    "      resnet_v2_block('block3', base_depth=256, num_units=2, stride=2),\n",
    "      resnet_v2_block('block4', base_depth=512, num_units=2, stride=2),\n",
    "    ]\n",
    "    return resnet_v2(\n",
    "      inputs,\n",
    "      blocks,\n",
    "      num_classes,\n",
    "      is_training,\n",
    "      global_pool,\n",
    "      output_stride,\n",
    "      include_root_block,\n",
    "      reuse=reuse,\n",
    "      scope=scope)\n",
    "\n",
    "def make_ocr_net(inputs, num_classes, is_training=False):\n",
    "    '''\n",
    "    Creates neural network graph.\n",
    "    Image width halved and it's define timestamps width (feature sequence length) \n",
    "    No activation after output (no softmax), due to it's presence at ctc_loss() and beam_search().\n",
    "    After resnet head features are resized to be [batch,1,width,channel], and after that goes 1x1 conv \n",
    "    to make anology of dense connaction for each timestamp.\n",
    "    \n",
    "    input: batch of images\n",
    "    output: tensor of size [batch, time_stamps_width, num_classes]\n",
    "    '''\n",
    "    with tf.variable_scope('resnet_base', values=[inputs]) as sc:\n",
    "        with slim.arg_scope([slim.conv2d],\n",
    "                              activation_fn=None, normalizer_fn=None):\n",
    "            net = resnet_utils.conv2d_same(inputs, 64, 7, stride=2, scope='conv1') #root conv for resnet\n",
    "            #net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1') # due to enlarge of receptive field\n",
    "            net = resnet_v2_26_base(net, output_stride=1, is_training = is_training)[0] # ouput is a tuple of last tensor and all tensors \n",
    "    with tf.variable_scope('class_head', values=[net]) as sc:\n",
    "        net = tf.transpose(net, [0,3,1,2]) # next 4 lines due to column to channel reshape. [batch,c,h,w]\n",
    "        _,c,h,_ = net.get_shape() # depth of input to conv op tensor should be static (defined)\n",
    "        shape = tf.shape(net)\n",
    "        net = tf.reshape(net, [shape[0], c*h, 1, shape[3]])\n",
    "        net = tf.transpose(net,[0,2,3,1]) # back to [batch,h,w,c] = [batch,1,w,features*h]\n",
    "        net = layers_lib.conv2d(net, num_classes, [1, 1], activation_fn=None) #CTC got softmax [batch,1,w,num_classes]\n",
    "        net = tf.squeeze(net,1) #[batch,w,num_classes]\n",
    "        return net\n",
    "\n",
    "def ctc_loss_layer(sequence_labels, logits, sequence_length):\n",
    "    \"\"\"\n",
    "    Build CTC Loss layer for training\n",
    "    sequence_length is a list of siquences lengths, len(sequence_length) = batch_size.\n",
    "    In our case sequences can not be different size due to it origin of images batch, \n",
    "    which should be of equal size (e.g. padded)\n",
    "    \"\"\"\n",
    "    loss = tf.nn.ctc_loss( sequence_labels, \n",
    "                           logits, \n",
    "                           sequence_length,\n",
    "                           time_major=False,  # [batch_size, max_time, num_classes] for logits\n",
    "                           ignore_longer_outputs_than_inputs=True )\n",
    "    total_loss = tf.reduce_mean( loss )\n",
    "    return total_loss\n",
    "\n",
    "def get_training(sequence_labels, net_logits, sequence_length, \n",
    "                   learning_rate=1e-4, decay_steps=2**16, decay_rate=0.9, decay_staircase=False, \n",
    "                   momentum=0.9):\n",
    "    \"\"\"\n",
    "    Set up training ops\n",
    "    https://github.com/weinman/cnn_lstm_ctc_ocr/blob/master/src/model_fn.py\n",
    "    \"\"\"\n",
    "    with tf.name_scope( \"train\" ):\n",
    "        net_logits_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)        \n",
    "        loss = ctc_loss_layer(sequence_labels, net_logits, sequence_length) \n",
    "        # Update batch norm stats [http://stackoverflow.com/questions/43234667]\n",
    "        extra_update_ops = tf.get_collection( tf.GraphKeys.UPDATE_OPS )\n",
    "        with tf.control_dependencies( extra_update_ops ):\n",
    "            # Calculate the learning rate given the parameters\n",
    "            learning_rate_tensor = tf.train.exponential_decay(\n",
    "                learning_rate,\n",
    "                tf.train.get_global_step(),\n",
    "                decay_steps,\n",
    "                decay_rate,\n",
    "                staircase=decay_staircase,\n",
    "                name='learning_rate' )\n",
    "            optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate_tensor,\n",
    "                beta1=momentum )\n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step(),\n",
    "                learning_rate=learning_rate_tensor, \n",
    "                optimizer=optimizer,\n",
    "                variables=net_logits_vars)\n",
    "            tf.summary.scalar('learning_rate', learning_rate_tensor )\n",
    "    return train_op, loss\n",
    "\n",
    "def get_prediction(output_net, seq_len, merge_repeated=False):\n",
    "    '''\n",
    "    predict by using beam search\n",
    "    input: output_net - logits (without softmax) of net\n",
    "           seq_len - length of predicted sequence \n",
    "    '''\n",
    "    net = tf.transpose(output_net, [1, 0, 2]) #transpose to [time, batch, logits]\n",
    "    decoded, prob = tf.nn.ctc_beam_search_decoder(net, seq_len, merge_repeated=merge_repeated)\n",
    "    return decoded, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRModel(object):\n",
    "    def __init__(self, image_height, num_classes, is_training=True, learning_rate=1e-4,\n",
    "                decay_steps=2**16, decay_rate=0.9, decay_staircase=False, momentum=0.9):\n",
    "        self.image_height = image_height\n",
    "        self.num_classes = num_classes\n",
    "        self.is_training = is_training\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate \n",
    "        self.decay_staircase = decay_staircase\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.input_image_batch = tf.placeholder(shape=(None,self.image_height,None,3), dtype=tf.float32) #image heights should be 32\n",
    "#         self.feature_seq_length = tf.placeholder(tf.int32, [None])\n",
    "        self.feature_seq_length = tf.fill([tf.shape(self.input_image_batch)[0]], tf.shape(self.input_image_batch)[2]//2) #as we know effective stride\n",
    "        self.sequence_labels = tf.sparse_placeholder(tf.int32, name='label') #sparce tensor of [[batch_num,time_stamp_num], values, seq_length]\n",
    "        \n",
    "        \n",
    "        net = make_ocr_net(self.input_image_batch, self.num_classes, is_training=self.is_training)\n",
    "        self.train_op, self.loss = get_training(self.sequence_labels, net, self.feature_seq_length,\n",
    "                                                self.learning_rate, self.decay_steps, \n",
    "                                                self.decay_rate, self.decay_staircase, self.momentum)\n",
    "        self.prediction = get_prediction(net, self.feature_seq_length, merge_repeated=False) # tuple(decoded, prob). decoded - list of top paths. I use top1\n",
    "        self.lev_dist = tf.reduce_mean(tf.edit_distance(tf.cast(self.prediction[0][0], tf.int32), self.sequence_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = ' !\"%&()*+,-./0123456789:=ABCDEFGHIJKLMNOPQRSTUVWXYZ\\\\_abcdefghijklmnopqrstuvwxyz|~ЁАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№'\n",
    "char_to_indx = dict(zip(all_chars,range(len(all_chars))))\n",
    "num_classes = len(all_chars)\n",
    "def string_to_label(string):\n",
    "    label = [char_to_indx[s] for s in string]\n",
    "    return label\n",
    "\n",
    "def batch_to_sparse(batch, dtype=np.int32): #batch of words\n",
    "    '''\n",
    "    function return sparce represantance of labels.\n",
    "    input: batch - batch of words (List of words)\n",
    "    output: indices - list of indexes [batch_num,time_stamp_num]\n",
    "            values - list of char indexes shape [batch]\n",
    "            shape - shape of dense batch represantation\n",
    "    '''\n",
    "    assert isinstance(batch, list) or isinstance(batch, np.ndarray), 'batch should be a list or numpy array of strings'\n",
    "    indices = [] #[batch_num,w]\n",
    "    values = [] # char indx\n",
    "    for batch_num, word in enumerate(batch):\n",
    "        assert isinstance(word,str), 'batch element should be a string'\n",
    "        word_as_indx = string_to_label(word)\n",
    "        indices.extend([(batch_num,char_num) for char_num, char in enumerate(word_as_indx)])\n",
    "        values.extend([char for char_num, char in enumerate(word_as_indx)])\n",
    "    indices = np.asarray(indices, dtype=dtype)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.array([len(batch),indices.max(0)[1]+1], dtype=dtype)\n",
    "    return indices, values, shape\n",
    "def from_sparse_to_batch():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.train.create_global_step()\n",
    "    model = OCRModel(image_height=32, num_classes=num_classes)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95 95]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    image_vert_line = np.zeros((2,32,190,3))\n",
    "    image_vert_line[0,:,60,0] = 1\n",
    "    image_vert_line[1,:,60,0] = 1\n",
    "    print(model.feature_seq_length.eval(feed_dict={model.input_image_batch: image_vert_line}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.4\n",
      "2.0\n",
      "0.8\n",
      "0.6\n",
      "0.6\n",
      "0.6\n",
      "0.6\n",
      "0.6\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "0.0\n",
      "0.4\n",
      "0.4\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.summary.FileWriter('log', sess.graph)\n",
    "    sess.run(init)\n",
    "    image_vert_line = np.zeros((2,32,180,3))\n",
    "    image_vert_line[0,:,60,0] = 1\n",
    "    image_vert_line[1,:,60,0] = 1\n",
    "    batch = batch_to_sparse(['hello','hello'])\n",
    "    feed = {\n",
    "            model.input_image_batch: image_vert_line,\n",
    "#             model.feature_seq_length: [90, 90],\n",
    "            model.sequence_labels: batch\n",
    "       }\n",
    "    for _ in range(40):\n",
    "        _, loss_value, dist = sess.run([model.train_op, model.loss, model.lev_dist], feed_dict = feed)\n",
    "        print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
