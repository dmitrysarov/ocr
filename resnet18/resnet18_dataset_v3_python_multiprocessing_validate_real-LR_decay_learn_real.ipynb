{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os                                                                          \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"                                       \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"  \n",
    "from tensorflow.contrib.slim.nets import resnet_v2, resnet_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers as layers_lib\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.nn import ctc_loss, conv2d\n",
    "import numpy as np\n",
    "resnet_v2_block = resnet_v2.resnet_v2_block\n",
    "resnet_v2 = resnet_v2.resnet_v2\n",
    "from lev_dist import distance\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "RESNET_STRIDE = 1\n",
    "IMAGE_HEIGHT = 32\n",
    "VAL_BATCH_SIZE = 64\n",
    "VAL_SIZE = 2048\n",
    "LR_DECAY_TOLERANCE = 5\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v2_26_base(inputs,\n",
    "                 num_classes=None,\n",
    "                 is_training=True, # True - due to update batchnorm layers\n",
    "                 global_pool=False,\n",
    "                 output_stride=1, # effective stride \n",
    "                 reuse=None,\n",
    "                 include_root_block=False, #first conv layer. Removed due to max pool supression. We need large receprive field\n",
    "                 scope='resnet_v2_26'):\n",
    "  \n",
    "    \"\"\"\n",
    "    Tensorflow resnet_v2 use only bottleneck blocks (consist of 3 layers).\n",
    "    Thus, this resnet layer model consist of 26 layers.\n",
    "    I put stride = 2 on each block due to increase receptive field.\n",
    "\n",
    "    \"\"\"\n",
    "    blocks = [\n",
    "      resnet_v2_block('block1', base_depth=64, num_units=2, stride=2),\n",
    "      resnet_v2_block('block2', base_depth=128, num_units=2, stride=2),\n",
    "      resnet_v2_block('block3', base_depth=256, num_units=2, stride=2),\n",
    "      resnet_v2_block('block4', base_depth=512, num_units=2, stride=2),\n",
    "    ]\n",
    "    return resnet_v2(\n",
    "      inputs,\n",
    "      blocks,\n",
    "      num_classes,\n",
    "      is_training,\n",
    "      global_pool,\n",
    "      output_stride,\n",
    "      include_root_block,\n",
    "      reuse=reuse,\n",
    "      scope=scope)\n",
    "\n",
    "def make_ocr_net(inputs, num_classes, is_training=True):\n",
    "    '''\n",
    "    Creates neural network graph.\n",
    "    Image width halved and it's define timestamps width (feature sequence length) \n",
    "    No activation after output (no softmax), due to it's presence at ctc_loss() and beam_search().\n",
    "    After resnet head features are resized to be [batch,1,width,channel], and after that goes 1x1 conv \n",
    "    to make anology of dense connaction for each timestamp.\n",
    "    \n",
    "    input: batch of images\n",
    "    output: tensor of size [batch, time_stamps_width, num_classes]\n",
    "    '''\n",
    "    with tf.variable_scope('resnet_base', values=[inputs]) as sc:\n",
    "        with slim.arg_scope([slim.conv2d],\n",
    "                              activation_fn=None, normalizer_fn=None):\n",
    "            net = resnet_utils.conv2d_same(inputs, 64, 7, stride=2, scope='conv1') #root conv for resnet\n",
    "            #net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1') # due to enlarge of receptive field\n",
    "            net = resnet_v2_26_base(net, output_stride=1, is_training = is_training)[0] # ouput is a tuple of last tensor and all tensors \n",
    "    with tf.variable_scope('class_head', values=[net]) as sc:\n",
    "        net = tf.transpose(net, [0,3,1,2]) # next 4 lines due to column to channel reshape. [batch,c,h,w]\n",
    "        _,c,h,_ = net.get_shape() # depth of input to conv op tensor should be static (defined)\n",
    "        shape = tf.shape(net)\n",
    "        net = tf.reshape(net, [shape[0], c*h, 1, shape[3]])\n",
    "        net = tf.transpose(net,[0,2,3,1]) # back to [batch,h,w,c] = [batch,1,w,features*h]\n",
    "        net = layers_lib.conv2d(net, num_classes, [1, 1], activation_fn=None) #CTC got softmax [batch,1,w,num_classes]\n",
    "        net = tf.squeeze(net,1) #[batch,w,num_classes]\n",
    "        return net\n",
    "\n",
    "def ctc_loss_layer(sequence_labels, logits, sequence_length):\n",
    "    \"\"\"\n",
    "    Build CTC Loss layer for training\n",
    "    sequence_length is a list of siquences lengths, len(sequence_length) = batch_size.\n",
    "    In our case sequences can not be different size due to it origin of images batch, \n",
    "    which should be of equal size (e.g. padded)\n",
    "    \"\"\"\n",
    "#     logits = tf.Print(logits, [sequence_labels.dense_shape], message='sequence_labels ', summarize=10000)\n",
    "#     logits = tf.Print(logits, [tf.shape(logits)], message='logits ', summarize=10000)\n",
    "#     logits = tf.Print(logits, [sequence_length], message='sequence_length ', summarize=10000)\n",
    "\n",
    "    loss = tf.nn.ctc_loss( sequence_labels, \n",
    "                           logits, \n",
    "                           sequence_length,\n",
    "                           time_major=False,  # [batch_size, max_time, num_classes] for logits\n",
    "                           ctc_merge_repeated = False,\n",
    "                           ignore_longer_outputs_than_inputs=False,\n",
    "                           preprocess_collapse_repeated=False)\n",
    "    total_loss = tf.reduce_mean( loss )\n",
    "    return total_loss\n",
    "\n",
    "def get_training(sequence_labels, net_logits, sequence_length, \n",
    "                   learning_rate=1e-4, momentum=0.9):\n",
    "    \"\"\"\n",
    "    Set up training ops\n",
    "    https://github.com/weinman/cnn_lstm_ctc_ocr/blob/master/src/model_fn.py\n",
    "    \"\"\"\n",
    "    with tf.name_scope( \"train\" ):\n",
    "        net_logits_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)        \n",
    "        loss = ctc_loss_layer(sequence_labels, net_logits, sequence_length) \n",
    "        # Update batch norm stats [http://stackoverflow.com/questions/43234667]\n",
    "        extra_update_ops = tf.get_collection( tf.GraphKeys.UPDATE_OPS )\n",
    "        with tf.control_dependencies( extra_update_ops ):\n",
    "            learning_rate_tensor = learning_rate\n",
    "            optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate_tensor,\n",
    "                beta1=momentum )\n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step(),\n",
    "                learning_rate=learning_rate_tensor, \n",
    "                optimizer=optimizer,\n",
    "                variables=net_logits_vars)\n",
    "    return train_op, loss, learning_rate_tensor\n",
    "\n",
    "def get_prediction(output_net, seq_len, merge_repeated=False):\n",
    "    '''\n",
    "    predict by using beam search\n",
    "    input: output_net - logits (without softmax) of net\n",
    "           seq_len - length of predicted sequence \n",
    "    '''\n",
    "#     net = tf.transpose(output_net, [1, 0, 2]) #transpose to [time, batch, logits]\n",
    "#     decoded_sparse_list, prob = tf.nn.ctc_beam_search_decoder(net, seq_len, merge_repeated=merge_repeated)\n",
    "\n",
    "    decoded_dense = tf.argmax(output_net, -1)\n",
    "    decoded_sparse  = tf.contrib.layers.dense_to_sparse(decoded_dense)\n",
    "    decoded_sparse_list = [decoded_sparse]\n",
    "    prob = tf.ones(tf.shape(output_net)[0], 1) #batch size ,top_paths\n",
    "    return decoded_sparse_list, prob\n",
    "\n",
    "def get_min_max_dist_image_label_pred(input_image_batch, sequence_labels, distances, prediction, table):\n",
    "    sequence_labels_dense = tf.sparse_to_dense(sequence_labels.indices, sequence_labels.dense_shape, \n",
    "                                               sequence_labels.values)   \n",
    "#     distances = tf.Print(distances, [distances])\n",
    "    max_indx = tf.argmax(distances)\n",
    "    min_indx = tf.argmin(distances)\n",
    "    max_image = tf.expand_dims(tf.gather(input_image_batch, max_indx),0)\n",
    "    min_image = tf.expand_dims(tf.gather(input_image_batch, min_indx),0)\n",
    "    max_label = tf.gather(sequence_labels_dense, max_indx)\n",
    "    max_label_string = tf.reduce_join(table.lookup(tf.cast(max_label, tf.int64)-1))\n",
    "    max_label_string = tf.strings.regex_replace(max_label_string,'blank','')\n",
    "    min_label = tf.gather(sequence_labels_dense, min_indx)\n",
    "    min_label_string = tf.reduce_join(table.lookup(tf.cast(min_label, tf.int64)-1))\n",
    "    min_label_string = tf.strings.regex_replace(min_label_string,'blank','')\n",
    "    pred_dense = tf.sparse_to_dense(prediction[0][0].indices, prediction[0][0].dense_shape, \n",
    "                                               prediction[0][0].values)\n",
    "    max_prediction = tf.gather(pred_dense, max_indx)\n",
    "    max_prediction_string = tf.reduce_join(table.lookup(tf.cast(max_prediction, tf.int64)-1))\n",
    "    max_prediction_string = tf.strings.regex_replace(max_prediction_string,'blank','')\n",
    "    min_prediction = tf.gather(pred_dense, min_indx)\n",
    "    min_prediction_string = tf.reduce_join(table.lookup(tf.cast(min_prediction, tf.int64)-1))\n",
    "    min_prediction_string = tf.strings.regex_replace(min_prediction_string,'blank','')\n",
    "\n",
    "    with tf.name_scope('prediction_image'):\n",
    "        tf.summary.image('Max distance image', max_image)\n",
    "        tf.summary.image('Min distance image', min_image)\n",
    "        tf.summary.text('Max distance gt label', max_label_string)\n",
    "        tf.summary.text('Min distance gt label', min_label_string)\n",
    "        tf.summary.text('Max distance pred label', max_prediction_string)\n",
    "        tf.summary.text('Min distance pred label', min_prediction_string)\n",
    "    return tf.summary.merge_all(scope='prediction_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRModel(object):\n",
    "    def __init__(self, charset, input_image_batch, sequence_labels, is_training=True, learning_rate=1e-4,\n",
    "                momentum=0.9):\n",
    "        self.charset = charset\n",
    "        self.num_classes = len(charset)+2 #indexing starts from one, +empty char. Yes, I know that I got 0-th rudimental output, whatever\n",
    "        self.is_training = is_training\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        mapping_char = tf.constant(list(charset))\n",
    "        self.table = tf.contrib.lookup.index_to_string_table_from_tensor(\n",
    "                    mapping_char, default_value=\"blank\")\n",
    "        self.build(input_image_batch, sequence_labels)\n",
    "        \n",
    "    def build(self, input_image_batch, sequence_labels):\n",
    "        input_image_batch = tf.Print(input_image_batch, [tf.shape(input_image_batch)], summarize=10000, message='input image')\n",
    "        self.input_image_batch = input_image_batch\n",
    "        \n",
    "        self.sequence_labels = sequence_labels\n",
    "        self.feature_seq_length = tf.fill([tf.shape(self.input_image_batch)[0]], tf.shape(self.input_image_batch)[2]//(2*RESNET_STRIDE)) #as we know effective stride\n",
    "        \n",
    "        net = make_ocr_net(self.input_image_batch, self.num_classes, is_training=self.is_training)\n",
    "        self.net = net\n",
    "        self.train_op, self.loss, self.learning_rate_tensor = get_training(self.sequence_labels, net, self.feature_seq_length,\n",
    "                                                self.learning_rate, self.momentum)\n",
    "        self.prediction = get_prediction(net, self.feature_seq_length, merge_repeated=False) # tuple(decoded, prob). decoded - list of top paths. I use top1\n",
    "        lev_dist_batch = tf.edit_distance(tf.cast(self.prediction[0][0], tf.int32), self.sequence_labels)\n",
    "        self.lev_dist = tf.reduce_mean(lev_dist_batch)\n",
    "        pred_dense = tf.sparse_to_dense(self.prediction[0][0].indices, self.prediction[0][0].dense_shape, \n",
    "                                               self.prediction[0][0].values)\n",
    "        self.prediction_string = tf.reduce_join(self.table.lookup(tf.cast(pred_dense, tf.int64)-1), axis=1)\n",
    "        \n",
    "        with tf.name_scope('prediction_metrics'):\n",
    "            tf.summary.scalar('CTC loss', self.loss)\n",
    "            tf.summary.scalar('Levenshtein distance', self.lev_dist)\n",
    "            tf.summary.scalar('Learning rate', self.learning_rate_tensor)\n",
    "        self.merged_summary_metrics = tf.summary.merge_all(scope='prediction_metrics')\n",
    "        self.merged_summary_image = get_min_max_dist_image_label_pred(input_image_batch, sequence_labels, \n",
    "                                                                      lev_dist_batch, self.prediction, \n",
    "                                                                      self.table)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data_generator/valid_charset.txt', 'r', encoding = 'utf-8') as f:\n",
    "    valid_charset = f.read()\n",
    "all_chars = list(valid_charset)\n",
    "char_to_indx = dict(zip(all_chars,range(len(all_chars))))\n",
    "indx_to_char = dict(zip(range(len(all_chars)),all_chars))\n",
    "\n",
    "num_classes = len(all_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from skimage import io\n",
    "from skimage.transform import rescale\n",
    "from skimage.color import rgb2gray\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def resize_height(image, max_dim):\n",
    "    h, w = image.shape\n",
    "    scale_factor = max_dim/h\n",
    "    output_image = rescale(image.copy(), scale = scale_factor, order = 3)\n",
    "    return output_image\n",
    "\n",
    "def string_to_label(string):\n",
    "    label = [char_to_indx[s]+1 for s in string] #index start from 1\n",
    "    return np.array(label)\n",
    "\n",
    "def label_to_string(label):\n",
    "    label = [indx_to_char[s-1] for s in label] #index start from 1\n",
    "    return ''.join(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bins 100 is too hight for stratification. Lowering to 99\n",
      "number of bins 99 is too hight for stratification. Lowering to 98\n",
      "number of bins 98 is too hight for stratification. Lowering to 97\n",
      "number of bins 97 is too hight for stratification. Lowering to 96\n",
      "number of bins 96 is too hight for stratification. Lowering to 95\n",
      "number of bins 95 is too hight for stratification. Lowering to 94\n",
      "number of bins 94 is too hight for stratification. Lowering to 93\n",
      "number of bins 93 is too hight for stratification. Lowering to 92\n",
      "number of bins 92 is too hight for stratification. Lowering to 91\n",
      "number of bins 91 is too hight for stratification. Lowering to 90\n",
      "number of bins 90 is too hight for stratification. Lowering to 89\n",
      "number of bins 89 is too hight for stratification. Lowering to 88\n",
      "number of bins 88 is too hight for stratification. Lowering to 87\n",
      "number of bins 87 is too hight for stratification. Lowering to 86\n",
      "number of bins 86 is too hight for stratification. Lowering to 85\n",
      "number of bins 85 is too hight for stratification. Lowering to 84\n",
      "number of bins 84 is too hight for stratification. Lowering to 83\n",
      "number of bins 83 is too hight for stratification. Lowering to 82\n",
      "number of bins 82 is too hight for stratification. Lowering to 81\n",
      "number of bins 81 is too hight for stratification. Lowering to 80\n",
      "number of bins 80 is too hight for stratification. Lowering to 79\n",
      "number of bins 79 is too hight for stratification. Lowering to 78\n",
      "number of bins 78 is too hight for stratification. Lowering to 77\n",
      "number of bins 77 is too hight for stratification. Lowering to 76\n",
      "number of bins 76 is too hight for stratification. Lowering to 75\n",
      "number of bins 75 is too hight for stratification. Lowering to 74\n",
      "number of bins 74 is too hight for stratification. Lowering to 73\n",
      "number of bins 73 is too hight for stratification. Lowering to 72\n",
      "number of bins 72 is too hight for stratification. Lowering to 71\n",
      "number of bins 71 is too hight for stratification. Lowering to 70\n",
      "number of bins 70 is too hight for stratification. Lowering to 69\n",
      "number of bins 69 is too hight for stratification. Lowering to 68\n",
      "number of bins 68 is too hight for stratification. Lowering to 67\n",
      "number of bins 67 is too hight for stratification. Lowering to 66\n",
      "number of bins 66 is too hight for stratification. Lowering to 65\n",
      "number of bins 65 is too hight for stratification. Lowering to 64\n",
      "number of bins 64 is too hight for stratification. Lowering to 63\n",
      "number of bins 63 is too hight for stratification. Lowering to 62\n",
      "number of bins 62 is too hight for stratification. Lowering to 61\n",
      "number of bins 61 is too hight for stratification. Lowering to 60\n",
      "number of bins 60 is too hight for stratification. Lowering to 59\n",
      "number of bins 59 is too hight for stratification. Lowering to 58\n",
      "number of bins 58 is too hight for stratification. Lowering to 57\n",
      "number of bins 57 is too hight for stratification. Lowering to 56\n",
      "number of bins 56 is too hight for stratification. Lowering to 55\n",
      "number of bins 55 is too hight for stratification. Lowering to 54\n",
      "number of bins 54 is too hight for stratification. Lowering to 53\n",
      "number of bins 53 is too hight for stratification. Lowering to 52\n",
      "number of bins 52 is too hight for stratification. Lowering to 51\n",
      "number of bins 51 is too hight for stratification. Lowering to 50\n",
      "number of bins 50 is too hight for stratification. Lowering to 49\n",
      "number of bins 49 is too hight for stratification. Lowering to 48\n",
      "number of bins 48 is too hight for stratification. Lowering to 47\n",
      "number of bins 47 is too hight for stratification. Lowering to 46\n",
      "number of bins 46 is too hight for stratification. Lowering to 45\n",
      "number of bins 45 is too hight for stratification. Lowering to 44\n",
      "number of bins 44 is too hight for stratification. Lowering to 43\n",
      "number of bins 43 is too hight for stratification. Lowering to 42\n",
      "number of bins 42 is too hight for stratification. Lowering to 41\n",
      "number of bins 41 is too hight for stratification. Lowering to 40\n",
      "number of bins 40 is too hight for stratification. Lowering to 39\n",
      "number of bins 39 is too hight for stratification. Lowering to 38\n",
      "number of bins 38 is too hight for stratification. Lowering to 37\n",
      "number of bins 37 is too hight for stratification. Lowering to 36\n",
      "number of bins 36 is too hight for stratification. Lowering to 35\n",
      "number of bins 35 is too hight for stratification. Lowering to 34\n",
      "number of bins 34 is too hight for stratification. Lowering to 33\n",
      "number of bins 33 is too hight for stratification. Lowering to 32\n",
      "number of bins 32 is too hight for stratification. Lowering to 31\n",
      "number of bins 31 is too hight for stratification. Lowering to 30\n",
      "number of bins 30 is too hight for stratification. Lowering to 29\n",
      "number of bins 29 is too hight for stratification. Lowering to 28\n",
      "number of bins 28 is too hight for stratification. Lowering to 27\n",
      "number of bins 27 is too hight for stratification. Lowering to 26\n",
      "number of bins 26 is too hight for stratification. Lowering to 25\n",
      "number of bins 25 is too hight for stratification. Lowering to 24\n",
      "number of bins 24 is too hight for stratification. Lowering to 23\n",
      "number of bins 23 is too hight for stratification. Lowering to 22\n",
      "number of bins 22 is too hight for stratification. Lowering to 21\n",
      "number of bins 21 is too hight for stratification. Lowering to 20\n",
      "number of bins 20 is too hight for stratification. Lowering to 19\n",
      "number of bins 19 is too hight for stratification. Lowering to 18\n",
      "number of bins 18 is too hight for stratification. Lowering to 17\n",
      "number of bins 17 is too hight for stratification. Lowering to 16\n",
      "number of bins 16 is too hight for stratification. Lowering to 15\n",
      "number of bins 15 is too hight for stratification. Lowering to 14\n",
      "number of bins 14 is too hight for stratification. Lowering to 13\n",
      "number of bins 13 is too hight for stratification. Lowering to 12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('../datasets/labeled_strings.json', 'r') as f:\n",
    "    string_data = json.load(f)\n",
    "\n",
    "def my_stratify_value(data, num_of_bins):\n",
    "    '''\n",
    "    :param data: is a pair sequence of image path and string\n",
    "    '''\n",
    "    str_lens = []\n",
    "    for example in data:\n",
    "        string = example['context']\n",
    "        str_lens.append(len(string))\n",
    "    bins = np.linspace(min(str_lens), max(str_lens)+1, num_of_bins+1)\n",
    "    stratify_values = np.digitize(str_lens, bins)\n",
    "    return stratify_values\n",
    "\n",
    "def stratify_split_train_test(data, portion, func):\n",
    "    '''\n",
    "    :param func: function which return value (based on data example)  to be used in stratification of dataset split\n",
    "    '''\n",
    "    num_of_bins = 100\n",
    "    while True:\n",
    "        stratify_values = func(data, num_of_bins)\n",
    "        try:\n",
    "            split = train_test_split(data, test_size=portion, stratify=stratify_values, random_state=42)\n",
    "        except ValueError:\n",
    "            print('number of bins {} is too hight for stratification. Lowering to {}'.format(num_of_bins, num_of_bins-1))\n",
    "            num_of_bins -= 1\n",
    "            continue\n",
    "        return split\n",
    "\n",
    "# string_data_tupels = [('../datasets/'+string_data[i]['path'], string_data[i]['context'])\n",
    "#                       for i in np.random.choice(range(len(string_data)), size=1024, replace=False)]\n",
    "string_data_tupels = stratify_split_train_test(string_data, VAL_SIZE, func = my_stratify_value)\n",
    "string_data_tupels_test = string_data_tupels[1] # test only taken\n",
    "string_data_tupels_test = [('../datasets/'+i['path'], i['context'])\n",
    "                      for i in string_data_tupels_test]\n",
    "string_data_tupels_train = string_data_tupels[0]\n",
    "string_data_tupels_train = [('../datasets/'+i['path'], i['context'])\n",
    "                      for i in string_data_tupels_train]\n",
    "\n",
    "def map_func_test(image_path, string):\n",
    "    image_path = image_path.decode('utf-8')\n",
    "    string = string.decode('utf-8')\n",
    "    image = io.imread(image_path)\n",
    "    image = rgb2gray(image)\n",
    "    image = resize_height(image, 32)\n",
    "    image = image - 0.5\n",
    "    label = string_to_label(string)\n",
    "    return np.expand_dims(image,-1).astype(np.float32), label.astype(np.int32)\n",
    "def tf_py_map_func_wrapper_test(args):\n",
    "    return tf.py_func(func=map_func_test,\n",
    "               inp=(args[0], args[1]),\n",
    "               Tout = (tf.float32, tf.int32))\n",
    "\n",
    "def map_func(image_path, string):\n",
    "    image_path = image_path.decode('utf-8')\n",
    "    string = string.decode('utf-8')\n",
    "    image = io.imread(image_path)\n",
    "    image = rgb2gray(image)\n",
    "    image = resize_height(image, 32)\n",
    "    if image.shape[1] > 256:\n",
    "        image = rescale(image.copy(), scale = (1, 256/image.shape[1]), order = 3, preserve_range=True,)\n",
    "    image = image - 0.5\n",
    "    aug_seq_distortion = iaa.Sequential([\n",
    "            iaa.Affine(rotate=(-2,2), \n",
    "                order=[1], \n",
    "                 scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, \n",
    "                 shear=(-5, 5),\n",
    "                mode='constant', cval=(0)),\n",
    "            iaa.PerspectiveTransform(scale=(0, 0.02)),\n",
    "        ])\n",
    "    image = aug_seq_distortion.augment_image(image)\n",
    "    label = string_to_label(string)\n",
    "    return np.expand_dims(image,-1).astype(np.float32), label.astype(np.int32)\n",
    "def tf_py_map_func_wrapper(args):\n",
    "    return tf.py_func(func=map_func,\n",
    "               inp=(args[0], args[1]),\n",
    "               Tout = (tf.float32, tf.int32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name CTC loss is illegal; using CTC_loss instead.\n",
      "INFO:tensorflow:Summary name Levenshtein distance is illegal; using Levenshtein_distance instead.\n",
      "INFO:tensorflow:Summary name Learning rate is illegal; using Learning_rate instead.\n",
      "INFO:tensorflow:Summary name Max distance image is illegal; using Max_distance_image instead.\n",
      "INFO:tensorflow:Summary name Min distance image is illegal; using Min_distance_image instead.\n",
      "INFO:tensorflow:Summary name Max distance gt label is illegal; using Max_distance_gt_label instead.\n",
      "INFO:tensorflow:Summary name Min distance gt label is illegal; using Min_distance_gt_label instead.\n",
      "INFO:tensorflow:Summary name Max distance pred label is illegal; using Max_distance_pred_label instead.\n",
      "INFO:tensorflow:Summary name Min distance pred label is illegal; using Min_distance_pred_label instead.\n",
      "INFO:tensorflow:Summary name resnet_base/conv1/weights:0 is illegal; using resnet_base/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/conv1/biases:0 is illegal; using resnet_base/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/shortcut/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/shortcut/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/shortcut/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/shortcut/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/shortcut/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/shortcut/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/shortcut/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/shortcut/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/shortcut/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/shortcut/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/shortcut/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/shortcut/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv1/biases_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/shortcut/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/shortcut/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/shortcut/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/shortcut/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/postnorm/beta:0 is illegal; using resnet_base/resnet_v2_26/postnorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name class_head/Conv/weights:0 is illegal; using class_head/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name class_head/Conv/biases:0 is illegal; using class_head/Conv/biases_0 instead.\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "#     training_dataset = tf.data.Dataset().from_generator(local_data_generator, output_types= (tf.float32, tf.int32),\n",
    "#                                               output_shapes = (tf.TensorShape([None, IMAGE_HEIGHT, None,1]), \n",
    "#                                                                           (tf.TensorShape([None, None]))))\n",
    "#     training_dataset = training_dataset.prefetch(BATCH_SIZE)\n",
    "\n",
    "    string_data_tupels_ph_train = tf.placeholder(tf.string, shape=[None, None])\n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices(string_data_tupels_ph_train).shuffle(len(string_data_tupels_train)).repeat(-1)\n",
    "    training_dataset = training_dataset.map(tf_py_map_func_wrapper, num_parallel_calls=BATCH_SIZE).prefetch(BATCH_SIZE)\n",
    "    training_dataset = training_dataset.padded_batch(BATCH_SIZE, padded_shapes=([IMAGE_HEIGHT, None,1],[None]))\n",
    "    training_dataset = training_dataset.prefetch(BATCH_SIZE)\n",
    "    \n",
    "    string_data_tupels_ph_test = tf.placeholder(tf.string, shape=[None, None])\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices(string_data_tupels_ph_test).repeat(-1)\n",
    "\n",
    "    validation_dataset = validation_dataset.map(tf_py_map_func_wrapper_test, num_parallel_calls=VAL_BATCH_SIZE).prefetch(VAL_BATCH_SIZE)\n",
    "    validation_dataset = validation_dataset.padded_batch(VAL_BATCH_SIZE, padded_shapes=([IMAGE_HEIGHT, None,1],[None]))\n",
    "    validation_dataset = validation_dataset.prefetch(VAL_BATCH_SIZE)\n",
    "    \n",
    "    handle = tf.placeholder(tf.string, shape=[], name='iterator_handler')\n",
    "    iterator = tf.data.Iterator.from_string_handle(handle, training_dataset.output_types, training_dataset.output_shapes)\n",
    "    features, labels = iterator.get_next()\n",
    "    labels_sparce = tf.contrib.layers.dense_to_sparse(labels)\n",
    "    \n",
    "    training_iterator = training_dataset.make_initializable_iterator()    \n",
    "    validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "    \n",
    "    tf.train.create_global_step()\n",
    "\n",
    "    is_training = tf.placeholder_with_default(1, shape=[])\n",
    "    lr = tf.placeholder(shape=[], dtype=tf.float32)\n",
    "    \n",
    "    model = OCRModel(charset=all_chars, input_image_batch=features, \n",
    "                     sequence_labels=labels_sparce, is_training=tf.cast(is_training, tf.bool), learning_rate=lr,) #lr lowered\n",
    "    init = tf.global_variables_initializer()\n",
    "    table_init = tf.tables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    hists = [tf.summary.histogram(name=v.name, values=v.value()) for v in variables]\n",
    "    merged_summary_hists=tf.summary.merge(hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOG_DIR = 'log_real_data_learn'\n",
    "# with tf.Session(graph=graph) as sess:\n",
    "#     sess.run([init, training_iterator.initializer, validation_iterator.initializer, table_init],\n",
    "#              {string_data_tupels_ph_test: string_data_tupels_test, \n",
    "#               string_data_tupels_ph_train: string_data_tupels_train})\n",
    "#     training_handle = sess.run(training_iterator.string_handle())\n",
    "#     validation_handle = sess.run(validation_iterator.string_handle())\n",
    "#     train_image = sess.run(features,\n",
    "#                     {handle: training_handle})\n",
    "#     test_image = sess.run(features,\n",
    "#                     {handle: validation_handle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('train images')\n",
    "# for img in np.squeeze(train_image):\n",
    "#     plt.imshow(img, cmap='gray')\n",
    "#     plt.show()\n",
    "\n",
    "# print('test images')\n",
    "# for img in np.squeeze(test_image):\n",
    "#     plt.imshow(img, cmap='gray')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log_real_data_learn_2/model_lev_dist_0.02814469467609266.ckpt\n",
      "0.029387004863126676\n",
      "0.03202952113676935\n",
      "0.030573179875688736\n",
      "0.02904998882622035\n",
      "0.028858666787546408\n",
      "0.027345593498446133\n",
      "0.026286025077569927\n",
      "0.02812201973059315\n",
      "0.02667576866847899\n",
      "0.026311737891976442\n",
      "0.028018234822572367\n",
      "0.028359570418489546\n",
      "0.02666832844938912\n",
      "Lowering LR\n",
      "0.02510486060870492\n",
      "0.024818802445251007\n",
      "0.02450075022663869\n",
      "0.025328173705505722\n",
      "0.024484029780811663\n",
      "0.025181526542147094\n",
      "0.02517158514783704\n",
      "0.02561931467161736\n",
      "0.025536178748484534\n",
      "0.024814008197597732\n",
      "0.025285770106652332\n",
      "Lowering LR\n",
      "0.024893477285099955\n",
      "0.024270578127746203\n",
      "0.025501277706153146\n",
      "0.024917625493555043\n",
      "0.02455231349669091\n",
      "0.02430297105512528\n",
      "0.023199581752135785\n",
      "0.023854108043362313\n",
      "0.025000847563736564\n",
      "0.022959962329258038\n",
      "0.024124727397808007\n",
      "0.024648963577966473\n",
      "0.024038862970686064\n",
      "0.02430807019072245\n",
      "0.024459657538789835\n",
      "0.02325284491027843\n",
      "Lowering LR\n",
      "0.024308551206996556\n",
      "0.023789255316442447\n",
      "0.023675099125345878\n",
      "0.022212269778200858\n",
      "0.02462639127442811\n",
      "0.023518166245467615\n",
      "0.023042484348491564\n",
      "0.022998020278559198\n",
      "0.024317061021881605\n",
      "0.024522783997707895\n",
      "Lowering LR\n",
      "0.023970880577317853\n",
      "0.02484412541238571\n",
      "0.024486501291078756\n",
      "0.024778044428326526\n",
      "0.02566320715750103\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.python.client import timeline\n",
    "\n",
    "LOG_DIR = 'log_real_data_learn_2'\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR,'train'), sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "    pred_lev_dist = np.Inf\n",
    "                \n",
    "    sess.run([init, training_iterator.initializer, validation_iterator.initializer, table_init],\n",
    "             {string_data_tupels_ph_test: string_data_tupels_test, \n",
    "              string_data_tupels_ph_train: string_data_tupels_train})\n",
    "    \n",
    "    training_handle = sess.run(training_iterator.string_handle())\n",
    "    validation_handle = sess.run(validation_iterator.string_handle())\n",
    "    \n",
    "    pred_lev_dist = np.Inf\n",
    "    count_to_decay = 0\n",
    "    try:\n",
    "        saver.restore(sess, os.path.join(LOG_DIR,\"model_lev_dist_0.02814469467609266.ckpt\"))\n",
    "    except:\n",
    "        print('cant restore')\n",
    "    for num in range(int(1e10)):\n",
    "        \n",
    "        _, features_value = sess.run([model.train_op, features], {handle: training_handle,\n",
    "                                        lr: LR})\n",
    "        if num%1000 == 0:\n",
    "            _, ms = sess.run([model.train_op, model.merged_summary_metrics],\n",
    "                                      {handle: training_handle,\n",
    "                                       lr: LR})\n",
    "            train_writer.add_summary(ms, num)\n",
    "            train_writer.flush()          \n",
    "            \n",
    "            ms, ms_img = sess.run([model.merged_summary_metrics, model.merged_summary_image], \n",
    "                                   {handle: validation_handle,\n",
    "                                    lr: LR})\n",
    "            test_writer.add_summary(ms, num)\n",
    "            test_writer.add_summary(ms_img, num)\n",
    "            test_writer.flush()\n",
    "            \n",
    "            lev_dist = []\n",
    "            for _ in range(int(VAL_SIZE/VAL_BATCH_SIZE)):\n",
    "                pred_strings, gt_labels = sess.run([model.prediction_string, labels], {handle: validation_handle,\n",
    "                                                                                      is_training: 0, \n",
    "                                                                                      lr: LR})\n",
    "                pred_strings = [pr.decode('utf-8').replace('blank','').upper() for pr in pred_strings]\n",
    "                gt_string = [label_to_string(gl[gl!=0]) for gl in list(gt_labels)]\n",
    "                lev_dist.append(np.mean([distance(ps, gs) for ps, gs in zip(pred_strings, gt_string)]))\n",
    "            lev_dist = np.mean(lev_dist)\n",
    "            print(lev_dist)\n",
    "            if lev_dist < pred_lev_dist:\n",
    "                pred_lev_dist = lev_dist\n",
    "                saver.save(sess, os.path.join(LOG_DIR, \"model_lev_dist_{}.ckpt\".format(lev_dist))) \n",
    "                count_to_decay = 0\n",
    "            else:\n",
    "                count_to_decay += 1\n",
    "            if count_to_decay>LR_DECAY_TOLERANCE:\n",
    "                print('Lowering LR')\n",
    "                count_to_decay = 0\n",
    "                LR /= 2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
