{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os                                                                          \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"                                       \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  \n",
    "from tensorflow.contrib.slim.nets import resnet_v2, resnet_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers as layers_lib\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.nn import ctc_loss, conv2d\n",
    "import numpy as np\n",
    "resnet_v2_block = resnet_v2.resnet_v2_block\n",
    "resnet_v2 = resnet_v2.resnet_v2\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "RESNET_STRIDE = 1\n",
    "IMAGE_HEIGHT = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v2_26_base(inputs,\n",
    "                 num_classes=None,\n",
    "                 is_training=True, # True - due to update batchnorm layers\n",
    "                 global_pool=False,\n",
    "                 output_stride=1, # effective stride \n",
    "                 reuse=None,\n",
    "                 include_root_block=False, #first conv layer. Removed due to max pool supression. We need large receprive field\n",
    "                 scope='resnet_v2_26'):\n",
    "  \n",
    "    \"\"\"\n",
    "    Tensorflow resnet_v2 use only bottleneck blocks (consist of 3 layers).\n",
    "    Thus, this resnet layer model consist of 26 layers.\n",
    "    I put stride = 2 on each block due to increase receptive field.\n",
    "\n",
    "    \"\"\"\n",
    "    blocks = [\n",
    "      resnet_v2_block('block1', base_depth=64, num_units=2, stride=2),\n",
    "      resnet_v2_block('block2', base_depth=128, num_units=2, stride=2),\n",
    "      resnet_v2_block('block3', base_depth=256, num_units=2, stride=2),\n",
    "      resnet_v2_block('block4', base_depth=512, num_units=2, stride=2),\n",
    "    ]\n",
    "    return resnet_v2(\n",
    "      inputs,\n",
    "      blocks,\n",
    "      num_classes,\n",
    "      is_training,\n",
    "      global_pool,\n",
    "      output_stride,\n",
    "      include_root_block,\n",
    "      reuse=reuse,\n",
    "      scope=scope)\n",
    "\n",
    "def make_ocr_net(inputs, num_classes, is_training=False):\n",
    "    '''\n",
    "    Creates neural network graph.\n",
    "    Image width halved and it's define timestamps width (feature sequence length) \n",
    "    No activation after output (no softmax), due to it's presence at ctc_loss() and beam_search().\n",
    "    After resnet head features are resized to be [batch,1,width,channel], and after that goes 1x1 conv \n",
    "    to make anology of dense connaction for each timestamp.\n",
    "    \n",
    "    input: batch of images\n",
    "    output: tensor of size [batch, time_stamps_width, num_classes]\n",
    "    '''\n",
    "    with tf.variable_scope('resnet_base', values=[inputs]) as sc:\n",
    "        with slim.arg_scope([slim.conv2d],\n",
    "                              activation_fn=None, normalizer_fn=None):\n",
    "            net = resnet_utils.conv2d_same(inputs, 64, 7, stride=2, scope='conv1') #root conv for resnet\n",
    "            #net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1') # due to enlarge of receptive field\n",
    "            net = resnet_v2_26_base(net, output_stride=1, is_training = is_training)[0] # ouput is a tuple of last tensor and all tensors \n",
    "    with tf.variable_scope('class_head', values=[net]) as sc:\n",
    "        net = tf.transpose(net, [0,3,1,2]) # next 4 lines due to column to channel reshape. [batch,c,h,w]\n",
    "        _,c,h,_ = net.get_shape() # depth of input to conv op tensor should be static (defined)\n",
    "        shape = tf.shape(net)\n",
    "        net = tf.reshape(net, [shape[0], c*h, 1, shape[3]])\n",
    "        net = tf.transpose(net,[0,2,3,1]) # back to [batch,h,w,c] = [batch,1,w,features*h]\n",
    "        net = layers_lib.conv2d(net, num_classes, [1, 1], activation_fn=None) #CTC got softmax [batch,1,w,num_classes]\n",
    "        net = tf.squeeze(net,1) #[batch,w,num_classes]\n",
    "        return net\n",
    "\n",
    "def ctc_loss_layer(sequence_labels, logits, sequence_length):\n",
    "    \"\"\"\n",
    "    Build CTC Loss layer for training\n",
    "    sequence_length is a list of siquences lengths, len(sequence_length) = batch_size.\n",
    "    In our case sequences can not be different size due to it origin of images batch, \n",
    "    which should be of equal size (e.g. padded)\n",
    "    \"\"\"\n",
    "    loss = tf.nn.ctc_loss( sequence_labels, \n",
    "                           logits, \n",
    "                           sequence_length,\n",
    "                           time_major=False,  # [batch_size, max_time, num_classes] for logits\n",
    "                           ignore_longer_outputs_than_inputs=True )\n",
    "    total_loss = tf.reduce_mean( loss )\n",
    "    return total_loss\n",
    "\n",
    "def get_training(sequence_labels, net_logits, sequence_length, \n",
    "                   learning_rate=1e-4, decay_steps=2**16, decay_rate=0.9, decay_staircase=False, \n",
    "                   momentum=0.9):\n",
    "    \"\"\"\n",
    "    Set up training ops\n",
    "    https://github.com/weinman/cnn_lstm_ctc_ocr/blob/master/src/model_fn.py\n",
    "    \"\"\"\n",
    "    with tf.name_scope( \"train\" ):\n",
    "        net_logits_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)        \n",
    "        loss = ctc_loss_layer(sequence_labels, net_logits, sequence_length) \n",
    "        # Update batch norm stats [http://stackoverflow.com/questions/43234667]\n",
    "        extra_update_ops = tf.get_collection( tf.GraphKeys.UPDATE_OPS )\n",
    "        with tf.control_dependencies( extra_update_ops ):\n",
    "            # Calculate the learning rate given the parameters\n",
    "            learning_rate_tensor = tf.train.exponential_decay(\n",
    "                learning_rate,\n",
    "                tf.train.get_global_step(),\n",
    "                decay_steps,\n",
    "                decay_rate,\n",
    "                staircase=decay_staircase,\n",
    "                name='learning_rate' )\n",
    "            optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate_tensor,\n",
    "                beta1=momentum )\n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step(),\n",
    "                learning_rate=learning_rate_tensor, \n",
    "                optimizer=optimizer,\n",
    "                variables=net_logits_vars)\n",
    "            tf.summary.scalar('learning_rate', learning_rate_tensor )\n",
    "    return train_op, loss, learning_rate_tensor\n",
    "\n",
    "def get_prediction(output_net, seq_len, merge_repeated=False):\n",
    "    '''\n",
    "    predict by using beam search\n",
    "    input: output_net - logits (without softmax) of net\n",
    "           seq_len - length of predicted sequence \n",
    "    '''\n",
    "    net = tf.transpose(output_net, [1, 0, 2]) #transpose to [time, batch, logits]\n",
    "    decoded, prob = tf.nn.ctc_beam_search_decoder(net, seq_len, merge_repeated=merge_repeated)\n",
    "    return decoded, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRModel(object):\n",
    "    def __init__(self, image_height, num_classes, input_image_batch, sequence_labels, is_training=True, learning_rate=1e-4,\n",
    "                decay_steps=2**16, decay_rate=0.9, decay_staircase=False, momentum=0.9):\n",
    "        self.image_height = image_height\n",
    "        self.num_classes = num_classes\n",
    "        self.is_training = is_training\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate \n",
    "        self.decay_staircase = decay_staircase\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.build(input_image_batch, sequence_labels)\n",
    "        \n",
    "    def build(self, input_image_batch, sequence_labels):\n",
    "#         self.input_image_batch = tf.placeholder(shape=(None,self.image_height,None,3), dtype=tf.float32) #image heights should be 32\n",
    "#         self.feature_seq_length = 1024/32#tf.placeholder(tf.int32, [None])\n",
    "#         self.sequence_labels = tf.sparse_placeholder(tf.int32, name='label') #sparce tensor of [[batch_num,time_stamp_num], values, seq_length]\n",
    "        \n",
    "        self.input_image_batch = input_image_batch\n",
    "        self.sequence_labels = sequence_labels\n",
    "        self.feature_seq_length = tf.fill([tf.shape(self.input_image_batch)[0]], tf.shape(self.input_image_batch)[2]//(2*RESNET_STRIDE)) #as we know effective stride\n",
    "        \n",
    "        net = make_ocr_net(self.input_image_batch, self.num_classes, is_training=self.is_training)\n",
    "        self.net = net\n",
    "        self.train_op, self.loss, self.learning_rate_tensor = get_training(self.sequence_labels, net, self.feature_seq_length,\n",
    "                                                self.learning_rate, self.decay_steps, \n",
    "                                                self.decay_rate, self.decay_staircase, self.momentum)\n",
    "        self.prediction = get_prediction(net, self.feature_seq_length, merge_repeated=False) # tuple(decoded, prob). decoded - list of top paths. I use top1\n",
    "        self.lev_dist = tf.reduce_mean(tf.edit_distance(tf.cast(self.prediction[0][0], tf.int32), self.sequence_labels))\n",
    "        tf.summary.scalar('CTC loss', self.loss)\n",
    "        tf.summary.scalar('Levenshtein distance', self.lev_dist)\n",
    "        tf.summary.scalar('Learning rate', self.learning_rate_tensor)\n",
    "        self.merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../data_generator/')\n",
    "from data_generator import data_generator\n",
    "dg = data_generator(backgrounds_path='../../data_generator/backgrounds/', \n",
    "                                   fonts_path='../../data_generator/valid_fonts/',\n",
    "                                   valid_charset_path='../../data_generator/valid_charset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_data_generator():\n",
    "    while True:\n",
    "        yield dg.get_image_and_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_chars = ' !\"%&()*+,-./0123456789:=ABCDEFGHIJKLMNOPQRSTUVWXYZ\\\\_abcdefghijklmnopqrstuvwxyz|~ЁАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№'\n",
    "all_chars = list(set(dg.valid_charset))\n",
    "char_to_indx = dict(zip(all_chars,range(len(all_chars))))\n",
    "num_classes = len(all_chars)\n",
    "# def string_to_label(string):\n",
    "#     label = [char_to_indx[s] for s in string]\n",
    "#     return label\n",
    "\n",
    "# def batch_to_sparse(batch, dtype=np.int32): #batch of words\n",
    "#     '''\n",
    "#     function return sparce represantance of labels.\n",
    "#     input: batch - batch of words (List of words)\n",
    "#     output: indices - list of indexes [batch_num,time_stamp_num]\n",
    "#             values - list of char indexes shape [batch]\n",
    "#             shape - shape of dense batch represantation\n",
    "#     '''\n",
    "#     assert isinstance(batch, list) or isinstance(batch, np.ndarray), 'batch should be a list or numpy array of strings'\n",
    "#     indices = [] #[batch_num,w]\n",
    "#     values = [] # char indx\n",
    "#     for batch_num, word in enumerate(batch):\n",
    "#         assert isinstance(word,str), 'batch element should be a string'\n",
    "#         word_as_indx = string_to_label(word)\n",
    "#         indices.extend([(batch_num,char_num) for char_num, char in enumerate(word_as_indx)])\n",
    "#         values.extend([char for char_num, char in enumerate(word_as_indx)])\n",
    "#     indices = np.asarray(indices, dtype=dtype)\n",
    "#     values = np.asarray(values, dtype=dtype)\n",
    "#     shape = np.array([len(batch),indices.max(0)[1]+1], dtype=dtype)\n",
    "#     return indices, values, shape\n",
    "# def from_sparse_to_batch():\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name CTC loss is illegal; using CTC_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name CTC loss is illegal; using CTC_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Levenshtein distance is illegal; using Levenshtein_distance instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Levenshtein distance is illegal; using Levenshtein_distance instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Learning rate is illegal; using Learning_rate instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Learning rate is illegal; using Learning_rate instead.\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "#     dataset = tf.data.Dataset().from_generator(dg.get_image_and_label, output_types= (tf.float32, (tf.int64,tf.int32,tf.int64)),\n",
    "#                                               output_shapes = (tf.TensorShape([None,32,None,1]), \n",
    "#                                                                           (tf.TensorShape([None,None]),\n",
    "#                                                                            tf.TensorShape([None]),\n",
    "#                                                                            tf.TensorShape([None]))))\n",
    "    dataset = tf.data.Dataset().from_generator(local_data_generator, output_types= (tf.float32, tf.int32),\n",
    "                                              output_shapes = (tf.TensorShape([IMAGE_HEIGHT,None,1]), \n",
    "                                                                          (tf.TensorShape([None]))))\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(4)\n",
    "    data_iter = dataset.make_initializable_iterator()\n",
    "    features, labels = data_iter.get_next()\n",
    "    labels = tf.contrib.layers.dense_to_sparse(labels)\n",
    "    tf.train.create_global_step()\n",
    "#     model = OCRModel(image_height=1024, num_classes=num_classes+1, input_image_batch=features, \n",
    "#                      sequence_labels=tf.SparseTensor(*labels), is_training=True)\n",
    "    model = OCRModel(image_height=1024, num_classes=num_classes+2, input_image_batch=features, \n",
    "                     sequence_labels=labels, is_training=True)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    train_writer = tf.summary.FileWriter('log/train',\n",
    "                                          sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('log/test')\n",
    "    sess.run([init, data_iter.initializer])\n",
    "    for num in range(int(1e6)):\n",
    "        print(num)\n",
    "        _, ms = sess.run([model.train_op, model.merged_summary])\n",
    "        train_writer.add_summary(ms, num)\n",
    "        if num%100 == 0:\n",
    "            _, ms_ = sess.run([model.train_op, model.merged_summary])\n",
    "            test_writer.add_summary(ms_, num)\n",
    "            saver.save(sess, \"log/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO images preprocces in datagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
