{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os                                                                          \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"                                       \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  \n",
    "from tensorflow.contrib.slim.nets import resnet_v2, resnet_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers as layers_lib\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.nn import ctc_loss, conv2d\n",
    "import numpy as np\n",
    "resnet_v2_block = resnet_v2.resnet_v2_block\n",
    "resnet_v2 = resnet_v2.resnet_v2\n",
    "from lev_dist import distance\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "RESNET_STRIDE = 1\n",
    "IMAGE_HEIGHT = 32\n",
    "VAL_BATCH_SIZE = 64\n",
    "VAL_SIZE = 2048\n",
    "LR_DECAY_TOLERANCE = 10\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v2_26_base(inputs,\n",
    "                 num_classes=None,\n",
    "                 is_training=True, # True - due to update batchnorm layers\n",
    "                 global_pool=False,\n",
    "                 output_stride=1, # effective stride \n",
    "                 reuse=None,\n",
    "                 include_root_block=False, #first conv layer. Removed due to max pool supression. We need large receprive field\n",
    "                 scope='resnet_v2_26'):\n",
    "  \n",
    "    \"\"\"\n",
    "    Tensorflow resnet_v2 use only bottleneck blocks (consist of 3 layers).\n",
    "    Thus, this resnet layer model consist of 26 layers.\n",
    "    I put stride = 2 on each block due to increase receptive field.\n",
    "\n",
    "    \"\"\"\n",
    "    blocks = [\n",
    "      resnet_v2_block('block1', base_depth=64, num_units=2, stride=2),\n",
    "      resnet_v2_block('block2', base_depth=128, num_units=2, stride=2),\n",
    "      resnet_v2_block('block3', base_depth=256, num_units=2, stride=2),\n",
    "      resnet_v2_block('block4', base_depth=512, num_units=2, stride=2),\n",
    "    ]\n",
    "    return resnet_v2(\n",
    "      inputs,\n",
    "      blocks,\n",
    "      num_classes,\n",
    "      is_training,\n",
    "      global_pool,\n",
    "      output_stride,\n",
    "      include_root_block,\n",
    "      reuse=reuse,\n",
    "      scope=scope)\n",
    "\n",
    "def make_ocr_net(inputs, num_classes, is_training=True):\n",
    "    '''\n",
    "    Creates neural network graph.\n",
    "    Image width halved and it's define timestamps width (feature sequence length) \n",
    "    No activation after output (no softmax), due to it's presence at ctc_loss() and beam_search().\n",
    "    After resnet head features are resized to be [batch,1,width,channel], and after that goes 1x1 conv \n",
    "    to make anology of dense connaction for each timestamp.\n",
    "    \n",
    "    input: batch of images\n",
    "    output: tensor of size [batch, time_stamps_width, num_classes]\n",
    "    '''\n",
    "    with tf.variable_scope('resnet_base', values=[inputs]) as sc:\n",
    "        with slim.arg_scope([slim.conv2d],\n",
    "                              activation_fn=None, normalizer_fn=None):\n",
    "            net = resnet_utils.conv2d_same(inputs, 64, 7, stride=2, scope='conv1') #root conv for resnet\n",
    "            #net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1') # due to enlarge of receptive field\n",
    "            net = resnet_v2_26_base(net, output_stride=1, is_training = is_training)[0] # ouput is a tuple of last tensor and all tensors \n",
    "    with tf.variable_scope('class_head', values=[net]) as sc:\n",
    "        net = tf.transpose(net, [0,3,1,2]) # next 4 lines due to column to channel reshape. [batch,c,h,w]\n",
    "        _,c,h,_ = net.get_shape() # depth of input to conv op tensor should be static (defined)\n",
    "        shape = tf.shape(net)\n",
    "        net = tf.reshape(net, [shape[0], c*h, 1, shape[3]])\n",
    "        net = tf.transpose(net,[0,2,3,1]) # back to [batch,h,w,c] = [batch,1,w,features*h]\n",
    "        net = layers_lib.conv2d(net, num_classes, [1, 1], activation_fn=None) #CTC got softmax [batch,1,w,num_classes]\n",
    "        net = tf.squeeze(net,1) #[batch,w,num_classes]\n",
    "        return net\n",
    "\n",
    "def ctc_loss_layer(sequence_labels, logits, sequence_length):\n",
    "    \"\"\"\n",
    "    Build CTC Loss layer for training\n",
    "    sequence_length is a list of siquences lengths, len(sequence_length) = batch_size.\n",
    "    In our case sequences can not be different size due to it origin of images batch, \n",
    "    which should be of equal size (e.g. padded)\n",
    "    \"\"\"\n",
    "#     logits = tf.Print(logits, [sequence_labels.dense_shape], message='sequence_labels ', summarize=10000)\n",
    "#     logits = tf.Print(logits, [tf.shape(logits)], message='logits ', summarize=10000)\n",
    "#     logits = tf.Print(logits, [sequence_length], message='sequence_length ', summarize=10000)\n",
    "\n",
    "    loss = tf.nn.ctc_loss( sequence_labels, \n",
    "                           logits, \n",
    "                           sequence_length,\n",
    "                           time_major=False,  # [batch_size, max_time, num_classes] for logits\n",
    "                           ctc_merge_repeated = False,\n",
    "                           ignore_longer_outputs_than_inputs=False,\n",
    "                           preprocess_collapse_repeated=False)\n",
    "    total_loss = tf.reduce_mean( loss )\n",
    "    return total_loss\n",
    "\n",
    "def get_training(sequence_labels, net_logits, sequence_length, \n",
    "                   learning_rate=1e-4, momentum=0.9):\n",
    "    \"\"\"\n",
    "    Set up training ops\n",
    "    https://github.com/weinman/cnn_lstm_ctc_ocr/blob/master/src/model_fn.py\n",
    "    \"\"\"\n",
    "    with tf.name_scope( \"train\" ):\n",
    "        net_logits_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)        \n",
    "        loss = ctc_loss_layer(sequence_labels, net_logits, sequence_length) \n",
    "        # Update batch norm stats [http://stackoverflow.com/questions/43234667]\n",
    "        extra_update_ops = tf.get_collection( tf.GraphKeys.UPDATE_OPS )\n",
    "        with tf.control_dependencies( extra_update_ops ):\n",
    "            learning_rate_tensor = learning_rate\n",
    "            optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate_tensor,\n",
    "                beta1=momentum )\n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step(),\n",
    "                learning_rate=learning_rate_tensor, \n",
    "                optimizer=optimizer,\n",
    "                variables=net_logits_vars)\n",
    "    return train_op, loss, learning_rate_tensor\n",
    "\n",
    "def get_prediction(output_net, seq_len, merge_repeated=False):\n",
    "    '''\n",
    "    predict by using beam search\n",
    "    input: output_net - logits (without softmax) of net\n",
    "           seq_len - length of predicted sequence \n",
    "    '''\n",
    "#     net = tf.transpose(output_net, [1, 0, 2]) #transpose to [time, batch, logits]\n",
    "#     decoded_sparse_list, prob = tf.nn.ctc_beam_search_decoder(net, seq_len, merge_repeated=merge_repeated)\n",
    "\n",
    "    decoded_dense = tf.argmax(output_net, -1)\n",
    "    decoded_sparse  = tf.contrib.layers.dense_to_sparse(decoded_dense)\n",
    "    decoded_sparse_list = [decoded_sparse]\n",
    "    prob = tf.ones(tf.shape(output_net)[0], 1) #batch size ,top_paths\n",
    "    return decoded_sparse_list, prob\n",
    "\n",
    "def get_min_max_dist_image_label_pred(input_image_batch, sequence_labels, distances, prediction, table):\n",
    "    sequence_labels_dense = tf.sparse_to_dense(sequence_labels.indices, sequence_labels.dense_shape, \n",
    "                                               sequence_labels.values)   \n",
    "#     distances = tf.Print(distances, [distances])\n",
    "    max_indx = tf.argmax(distances)\n",
    "    min_indx = tf.argmin(distances)\n",
    "    max_image = tf.expand_dims(tf.gather(input_image_batch, max_indx),0)\n",
    "    min_image = tf.expand_dims(tf.gather(input_image_batch, min_indx),0)\n",
    "    max_label = tf.gather(sequence_labels_dense, max_indx)\n",
    "    max_label_string = tf.reduce_join(table.lookup(tf.cast(max_label, tf.int64)-1))\n",
    "    max_label_string = tf.strings.regex_replace(max_label_string,'blank','')\n",
    "    min_label = tf.gather(sequence_labels_dense, min_indx)\n",
    "    min_label_string = tf.reduce_join(table.lookup(tf.cast(min_label, tf.int64)-1))\n",
    "    min_label_string = tf.strings.regex_replace(min_label_string,'blank','')\n",
    "    pred_dense = tf.sparse_to_dense(prediction[0][0].indices, prediction[0][0].dense_shape, \n",
    "                                               prediction[0][0].values)\n",
    "    max_prediction = tf.gather(pred_dense, max_indx)\n",
    "    max_prediction_string = tf.reduce_join(table.lookup(tf.cast(max_prediction, tf.int64)-1))\n",
    "    max_prediction_string = tf.strings.regex_replace(max_prediction_string,'blank','')\n",
    "    min_prediction = tf.gather(pred_dense, min_indx)\n",
    "    min_prediction_string = tf.reduce_join(table.lookup(tf.cast(min_prediction, tf.int64)-1))\n",
    "    min_prediction_string = tf.strings.regex_replace(min_prediction_string,'blank','')\n",
    "\n",
    "    with tf.name_scope('prediction_image'):\n",
    "        tf.summary.image('Max distance image', max_image)\n",
    "        tf.summary.image('Min distance image', min_image)\n",
    "        tf.summary.text('Max distance gt label', max_label_string)\n",
    "        tf.summary.text('Min distance gt label', min_label_string)\n",
    "        tf.summary.text('Max distance pred label', max_prediction_string)\n",
    "        tf.summary.text('Min distance pred label', min_prediction_string)\n",
    "    return tf.summary.merge_all(scope='prediction_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRModel(object):\n",
    "    def __init__(self, charset, input_image_batch, sequence_labels, is_training=True, learning_rate=1e-4,\n",
    "                momentum=0.9):\n",
    "        self.charset = charset\n",
    "        self.num_classes = len(charset)+2 #indexing starts from one, +empty char. Yes, I know that I got 0-th rudimental output, whatever\n",
    "        self.is_training = is_training\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        mapping_char = tf.constant(list(charset))\n",
    "        self.table = tf.contrib.lookup.index_to_string_table_from_tensor(\n",
    "                    mapping_char, default_value=\"blank\")\n",
    "        self.build(input_image_batch, sequence_labels)\n",
    "        \n",
    "    def build(self, input_image_batch, sequence_labels):\n",
    "      \n",
    "        self.input_image_batch = input_image_batch\n",
    "        self.sequence_labels = sequence_labels\n",
    "        self.feature_seq_length = tf.fill([tf.shape(self.input_image_batch)[0]], tf.shape(self.input_image_batch)[2]//(2*RESNET_STRIDE)) #as we know effective stride\n",
    "        \n",
    "        net = make_ocr_net(self.input_image_batch, self.num_classes, is_training=self.is_training)\n",
    "        self.net = net\n",
    "        self.train_op, self.loss, self.learning_rate_tensor = get_training(self.sequence_labels, net, self.feature_seq_length,\n",
    "                                                self.learning_rate, self.momentum)\n",
    "        self.prediction = get_prediction(net, self.feature_seq_length, merge_repeated=False) # tuple(decoded, prob). decoded - list of top paths. I use top1\n",
    "        lev_dist_batch = tf.edit_distance(tf.cast(self.prediction[0][0], tf.int32), self.sequence_labels)\n",
    "        self.lev_dist = tf.reduce_mean(lev_dist_batch)\n",
    "        pred_dense = tf.sparse_to_dense(self.prediction[0][0].indices, self.prediction[0][0].dense_shape, \n",
    "                                               self.prediction[0][0].values)\n",
    "        self.prediction_string = tf.reduce_join(self.table.lookup(tf.cast(pred_dense, tf.int64)-1), axis=1)\n",
    "        \n",
    "        with tf.name_scope('prediction_metrics'):\n",
    "            tf.summary.scalar('CTC loss', self.loss)\n",
    "            tf.summary.scalar('Levenshtein distance', self.lev_dist)\n",
    "            tf.summary.scalar('Learning rate', self.learning_rate_tensor)\n",
    "        self.merged_summary_metrics = tf.summary.merge_all(scope='prediction_metrics')\n",
    "        self.merged_summary_image = get_min_max_dist_image_label_pred(input_image_batch, sequence_labels, \n",
    "                                                                      lev_dist_batch, self.prediction, \n",
    "                                                                      self.table)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train (synthetic) data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-31 13:59:51,036:::data_generator:::1085 background files to produce data\n",
      "2019-01-31 13:59:51,037:::data_generator:::Caching background images...\n",
      "2019-01-31 14:00:36,917:::data_generator:::344 fonts to produce data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../data_generator/')\n",
    "from data_generator import data_generator\n",
    "dg = data_generator(backgrounds_path='../../data_generator/backgrounds/', \n",
    "                                   fonts_path='../../data_generator/valid_fonts/',\n",
    "                                   valid_charset_path='../../data_generator/valid_charset.txt', \n",
    "                                    background_type = ['const','real'],\n",
    "                   font_size_bound=(50, 25), max_string_lenght=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(*arg):\n",
    "    '''\n",
    "    we need this function to handle size of generator class which cannot be pickled \n",
    "    '''\n",
    "    image, label = dg.get_image_and_label()\n",
    "    image = image/255\n",
    "    image = image - 0.5\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Encounted empty string, substituting with 27,61583.094\n",
      "WARNING:root:Encounted empty string, substituting with 570148,6923.\n",
      "WARNING:root:Encounted empty string, substituting with .28579,41306\n",
      "WARNING:root:Encounted empty string, substituting with 27586931,40.\n",
      "WARNING:root:Encounted empty string, substituting with ,153089.4627\n",
      "WARNING:root:Encounted empty string, substituting with 75,08.391426\n",
      "WARNING:root:Encounted empty string, substituting with 70.,52981634\n",
      "WARNING:root:Encounted empty string, substituting with 4160358,.972\n",
      "WARNING:root:Encounted empty string, substituting with 96278,51430.\n",
      "WARNING:root:Encounted empty string, substituting with ,3186049257.\n",
      "WARNING:root:Encounted empty string, substituting with 2.390674185,\n",
      "WARNING:root:Encounted empty string, substituting with 83750,.62491\n",
      "WARNING:root:Encounted empty string, substituting with 239.418,0657\n",
      "WARNING:root:Encounted empty string, substituting with 7.190,826534\n",
      "WARNING:root:Encounted empty string, substituting with 9461835.7,20\n",
      "WARNING:root:Encounted empty string, substituting with 9,7.68203451\n",
      "WARNING:root:Encounted empty string, substituting with 8407152963.,\n",
      "WARNING:root:Encounted empty string, substituting with 52943.01,687\n",
      "WARNING:root:Encounted empty string, substituting with 679,081254.3\n",
      "WARNING:root:Encounted empty string, substituting with 36.12,847905\n",
      "WARNING:root:Encounted empty string, substituting with 345079,126.8\n",
      "WARNING:root:Encounted empty string, substituting with ,264390581.7\n",
      "WARNING:root:Encounted empty string, substituting with 4768,3.51920\n",
      "WARNING:root:Encounted empty string, substituting with 3.12058,6497\n",
      "WARNING:root:Encounted empty string, substituting with ,6314.970852\n",
      "WARNING:root:Encounted empty string, substituting with 5167429,.083\n",
      "WARNING:root:Encounted empty string, substituting with 48021.6,3579\n",
      "WARNING:root:Encounted empty string, substituting with 01,342.67895\n",
      "WARNING:root:Encounted empty string, substituting with 40.,65291783\n",
      "WARNING:root:Encounted empty string, substituting with 67.1920,3458\n",
      "WARNING:root:Encounted empty string, substituting with ,631.9780452\n",
      "WARNING:root:Encounted empty string, substituting with 9816,7243.50\n",
      "WARNING:root:Encounted empty string, substituting with 68790,3425.1\n",
      "WARNING:root:Encounted empty string, substituting with 7802594,1.63\n",
      "WARNING:root:Encounted empty string, substituting with 07,1529.4683\n",
      "WARNING:root:Encounted empty string, substituting with 37045,219.86\n",
      "WARNING:root:Encounted empty string, substituting with ,.1650728394\n",
      "WARNING:root:Encounted empty string, substituting with 7430598621,.\n",
      "WARNING:root:Encounted empty string, substituting with 097513,2.864\n",
      "WARNING:root:Encounted empty string, substituting with 4759.103,862\n",
      "WARNING:root:Encounted empty string, substituting with 7.124365,809\n",
      "WARNING:root:Encounted empty string, substituting with 8.746905,321\n",
      "WARNING:root:Encounted empty string, substituting with 734081,2.695\n",
      "WARNING:root:Encounted empty string, substituting with .95216,40837\n",
      "WARNING:root:Encounted empty string, substituting with ,0579412386.\n",
      "WARNING:root:Encounted empty string, substituting with 526048379.,1\n",
      "WARNING:root:Encounted empty string, substituting with 3.4178,09265\n",
      "WARNING:root:Encounted empty string, substituting with 2697.30,5481\n",
      "WARNING:root:Encounted empty string, substituting with 74981523.60,\n",
      "WARNING:root:Encounted empty string, substituting with .178465,3029\n",
      "WARNING:root:Encounted empty string, substituting with 3467089.,251\n",
      "WARNING:root:Encounted empty string, substituting with 4391.,627058\n",
      "WARNING:root:Encounted empty string, substituting with 89514.6320,7\n",
      "WARNING:root:Encounted empty string, substituting with 634,8159270.\n",
      "WARNING:root:Encounted empty string, substituting with 209.7,461835\n",
      "WARNING:root:Encounted empty string, substituting with 36905,2784.1\n",
      "WARNING:root:Encounted empty string, substituting with ,01463.79285\n",
      "WARNING:root:Encounted empty string, substituting with 1.890275,364\n",
      "WARNING:root:Encounted empty string, substituting with 08.579312,64\n",
      "WARNING:root:Encounted empty string, substituting with .63025714,89\n",
      "WARNING:root:Encounted empty string, substituting with 32740,651.89\n",
      "WARNING:root:Encounted empty string, substituting with 1,7.60295843\n",
      "WARNING:root:Encounted empty string, substituting with 39260.,51847\n",
      "WARNING:root:Encounted empty string, substituting with 951204,6.837\n",
      "WARNING:root:Encounted empty string, substituting with .,9120846573\n",
      "WARNING:root:Encounted empty string, substituting with 35.8460192,7\n",
      "WARNING:root:Encounted empty string, substituting with ,81.03749562\n",
      "WARNING:root:Encounted empty string, substituting with 385416.0,297\n",
      "WARNING:root:Encounted empty string, substituting with 1682973,04.5\n",
      "WARNING:root:Encounted empty string, substituting with 632408175.9,\n",
      "WARNING:root:Encounted empty string, substituting with 02,761.95348\n",
      "WARNING:root:Encounted empty string, substituting with 0364571,.289\n",
      "WARNING:root:Encounted empty string, substituting with 7,432051.968\n",
      "WARNING:root:Encounted empty string, substituting with 6.90715,4328\n",
      "WARNING:root:Encounted empty string, substituting with 54.3,9726108\n",
      "WARNING:root:Encounted empty string, substituting with .7023849,516\n",
      "WARNING:root:Encounted empty string, substituting with 374205.,1986\n",
      "WARNING:root:Encounted empty string, substituting with 40,.62819753\n",
      "WARNING:root:Encounted empty string, substituting with 729043,8.651\n",
      "WARNING:root:Encounted empty string, substituting with 7.,064281539\n",
      "WARNING:root:Encounted empty string, substituting with 251370.98,46\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-25:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-11:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-2:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-16:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-9:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-10:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-30:\n",
      "Process ForkPoolWorker-12:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-32:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-28:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "pool = multiprocessing.Pool(processes=32)\n",
    "def local_data_generator():\n",
    "    while True:\n",
    "        image_label_pairs = pool.map(helper, range(BATCH_SIZE))\n",
    "        image_batch, string_batch = zip(*image_label_pairs)\n",
    "        yield image_batch, string_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data_generator/valid_charset.txt', 'r', encoding = 'utf-8') as f:\n",
    "    valid_charset = f.read()\n",
    "all_chars = list(valid_charset)\n",
    "char_to_indx = dict(zip(all_chars,range(len(all_chars))))\n",
    "indx_to_char = dict(zip(range(len(all_chars)),all_chars))\n",
    "\n",
    "num_classes = len(all_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from skimage import io\n",
    "from skimage.transform import rescale\n",
    "from skimage.color import rgb2gray\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def resize_height(image, max_dim):\n",
    "    h, w = image.shape\n",
    "    scale_factor = max_dim/h\n",
    "    output_image = rescale(image.copy(), scale = scale_factor, order = 3)\n",
    "    return output_image\n",
    "\n",
    "def string_to_label(string):\n",
    "    label = [char_to_indx[s]+1 for s in string] #index start from 1\n",
    "    return np.array(label)\n",
    "\n",
    "def label_to_string(label):\n",
    "    label = [indx_to_char[s-1] for s in label] #index start from 1\n",
    "    return ''.join(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bins 100 is too hight for stratification. Lowering to 99\n",
      "number of bins 99 is too hight for stratification. Lowering to 98\n",
      "number of bins 98 is too hight for stratification. Lowering to 97\n",
      "number of bins 97 is too hight for stratification. Lowering to 96\n",
      "number of bins 96 is too hight for stratification. Lowering to 95\n",
      "number of bins 95 is too hight for stratification. Lowering to 94\n",
      "number of bins 94 is too hight for stratification. Lowering to 93\n",
      "number of bins 93 is too hight for stratification. Lowering to 92\n",
      "number of bins 92 is too hight for stratification. Lowering to 91\n",
      "number of bins 91 is too hight for stratification. Lowering to 90\n",
      "number of bins 90 is too hight for stratification. Lowering to 89\n",
      "number of bins 89 is too hight for stratification. Lowering to 88\n",
      "number of bins 88 is too hight for stratification. Lowering to 87\n",
      "number of bins 87 is too hight for stratification. Lowering to 86\n",
      "number of bins 86 is too hight for stratification. Lowering to 85\n",
      "number of bins 85 is too hight for stratification. Lowering to 84\n",
      "number of bins 84 is too hight for stratification. Lowering to 83\n",
      "number of bins 83 is too hight for stratification. Lowering to 82\n",
      "number of bins 82 is too hight for stratification. Lowering to 81\n",
      "number of bins 81 is too hight for stratification. Lowering to 80\n",
      "number of bins 80 is too hight for stratification. Lowering to 79\n",
      "number of bins 79 is too hight for stratification. Lowering to 78\n",
      "number of bins 78 is too hight for stratification. Lowering to 77\n",
      "number of bins 77 is too hight for stratification. Lowering to 76\n",
      "number of bins 76 is too hight for stratification. Lowering to 75\n",
      "number of bins 75 is too hight for stratification. Lowering to 74\n",
      "number of bins 74 is too hight for stratification. Lowering to 73\n",
      "number of bins 73 is too hight for stratification. Lowering to 72\n",
      "number of bins 72 is too hight for stratification. Lowering to 71\n",
      "number of bins 71 is too hight for stratification. Lowering to 70\n",
      "number of bins 70 is too hight for stratification. Lowering to 69\n",
      "number of bins 69 is too hight for stratification. Lowering to 68\n",
      "number of bins 68 is too hight for stratification. Lowering to 67\n",
      "number of bins 67 is too hight for stratification. Lowering to 66\n",
      "number of bins 66 is too hight for stratification. Lowering to 65\n",
      "number of bins 65 is too hight for stratification. Lowering to 64\n",
      "number of bins 64 is too hight for stratification. Lowering to 63\n",
      "number of bins 63 is too hight for stratification. Lowering to 62\n",
      "number of bins 62 is too hight for stratification. Lowering to 61\n",
      "number of bins 61 is too hight for stratification. Lowering to 60\n",
      "number of bins 60 is too hight for stratification. Lowering to 59\n",
      "number of bins 59 is too hight for stratification. Lowering to 58\n",
      "number of bins 58 is too hight for stratification. Lowering to 57\n",
      "number of bins 57 is too hight for stratification. Lowering to 56\n",
      "number of bins 56 is too hight for stratification. Lowering to 55\n",
      "number of bins 55 is too hight for stratification. Lowering to 54\n",
      "number of bins 54 is too hight for stratification. Lowering to 53\n",
      "number of bins 53 is too hight for stratification. Lowering to 52\n",
      "number of bins 52 is too hight for stratification. Lowering to 51\n",
      "number of bins 51 is too hight for stratification. Lowering to 50\n",
      "number of bins 50 is too hight for stratification. Lowering to 49\n",
      "number of bins 49 is too hight for stratification. Lowering to 48\n",
      "number of bins 48 is too hight for stratification. Lowering to 47\n",
      "number of bins 47 is too hight for stratification. Lowering to 46\n",
      "number of bins 46 is too hight for stratification. Lowering to 45\n",
      "number of bins 45 is too hight for stratification. Lowering to 44\n",
      "number of bins 44 is too hight for stratification. Lowering to 43\n",
      "number of bins 43 is too hight for stratification. Lowering to 42\n",
      "number of bins 42 is too hight for stratification. Lowering to 41\n",
      "number of bins 41 is too hight for stratification. Lowering to 40\n",
      "number of bins 40 is too hight for stratification. Lowering to 39\n",
      "number of bins 39 is too hight for stratification. Lowering to 38\n",
      "number of bins 38 is too hight for stratification. Lowering to 37\n",
      "number of bins 37 is too hight for stratification. Lowering to 36\n",
      "number of bins 36 is too hight for stratification. Lowering to 35\n",
      "number of bins 35 is too hight for stratification. Lowering to 34\n",
      "number of bins 34 is too hight for stratification. Lowering to 33\n",
      "number of bins 33 is too hight for stratification. Lowering to 32\n",
      "number of bins 32 is too hight for stratification. Lowering to 31\n",
      "number of bins 31 is too hight for stratification. Lowering to 30\n",
      "number of bins 30 is too hight for stratification. Lowering to 29\n",
      "number of bins 29 is too hight for stratification. Lowering to 28\n",
      "number of bins 28 is too hight for stratification. Lowering to 27\n",
      "number of bins 27 is too hight for stratification. Lowering to 26\n",
      "number of bins 26 is too hight for stratification. Lowering to 25\n",
      "number of bins 25 is too hight for stratification. Lowering to 24\n",
      "number of bins 24 is too hight for stratification. Lowering to 23\n",
      "number of bins 23 is too hight for stratification. Lowering to 22\n",
      "number of bins 22 is too hight for stratification. Lowering to 21\n",
      "number of bins 21 is too hight for stratification. Lowering to 20\n",
      "number of bins 20 is too hight for stratification. Lowering to 19\n",
      "number of bins 19 is too hight for stratification. Lowering to 18\n",
      "number of bins 18 is too hight for stratification. Lowering to 17\n",
      "number of bins 17 is too hight for stratification. Lowering to 16\n",
      "number of bins 16 is too hight for stratification. Lowering to 15\n",
      "number of bins 15 is too hight for stratification. Lowering to 14\n",
      "number of bins 14 is too hight for stratification. Lowering to 13\n",
      "number of bins 13 is too hight for stratification. Lowering to 12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('../datasets/labeled_strings.json', 'r') as f:\n",
    "    string_data = json.load(f)\n",
    "\n",
    "def my_stratify_value(data, num_of_bins):\n",
    "    '''\n",
    "    :param data: is a pair sequence of image path and string\n",
    "    '''\n",
    "    str_lens = []\n",
    "    for example in data:\n",
    "        string = example['context']\n",
    "        str_lens.append(len(string))\n",
    "    bins = np.linspace(min(str_lens), max(str_lens)+1, num_of_bins+1)\n",
    "    stratify_values = np.digitize(str_lens, bins)\n",
    "    return stratify_values\n",
    "\n",
    "def stratify_split_train_test(data, portion, func):\n",
    "    '''\n",
    "    :param func: function which return value (based on data example)  to be used in stratification of dataset split\n",
    "    '''\n",
    "    num_of_bins = 100\n",
    "    while True:\n",
    "        stratify_values = func(data, num_of_bins)\n",
    "        try:\n",
    "            split = train_test_split(data, test_size=portion, stratify=stratify_values)\n",
    "        except ValueError:\n",
    "            print('number of bins {} is too hight for stratification. Lowering to {}'.format(num_of_bins, num_of_bins-1))\n",
    "            num_of_bins -= 1\n",
    "            continue\n",
    "        return split\n",
    "\n",
    "# string_data_tupels = [('../datasets/'+string_data[i]['path'], string_data[i]['context'])\n",
    "#                       for i in np.random.choice(range(len(string_data)), size=1024, replace=False)]\n",
    "string_data_tupels = stratify_split_train_test(string_data, VAL_SIZE, func = my_stratify_value)[1] # test only taken\n",
    "string_data_tupels = [('../datasets/'+i['path'], i['context'])\n",
    "                      for i in string_data_tupels]\n",
    "def map_func(image_path, string):\n",
    "    image_path = image_path.decode('utf-8')\n",
    "    string = string.decode('utf-8')\n",
    "    image = io.imread(image_path)\n",
    "    image = rgb2gray(image)\n",
    "    image = resize_height(image, 32)\n",
    "    image = image - 0.5\n",
    "    label = string_to_label(string)\n",
    "    return np.expand_dims(image,-1).astype(np.float32), label.astype(np.int32)\n",
    "def tf_py_map_func_wrapper(args):\n",
    "    return tf.py_func(func=map_func,\n",
    "               inp=(args[0], args[1]),\n",
    "               Tout = (tf.float32, tf.int32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name CTC loss is illegal; using CTC_loss instead.\n",
      "INFO:tensorflow:Summary name Levenshtein distance is illegal; using Levenshtein_distance instead.\n",
      "INFO:tensorflow:Summary name Learning rate is illegal; using Learning_rate instead.\n",
      "INFO:tensorflow:Summary name Max distance image is illegal; using Max_distance_image instead.\n",
      "INFO:tensorflow:Summary name Min distance image is illegal; using Min_distance_image instead.\n",
      "INFO:tensorflow:Summary name Max distance gt label is illegal; using Max_distance_gt_label instead.\n",
      "INFO:tensorflow:Summary name Min distance gt label is illegal; using Min_distance_gt_label instead.\n",
      "INFO:tensorflow:Summary name Max distance pred label is illegal; using Max_distance_pred_label instead.\n",
      "INFO:tensorflow:Summary name Min distance pred label is illegal; using Min_distance_pred_label instead.\n",
      "INFO:tensorflow:Summary name resnet_base/conv1/weights:0 is illegal; using resnet_base/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/conv1/biases:0 is illegal; using resnet_base/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/shortcut/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/shortcut/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/shortcut/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/shortcut/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_1/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block1/unit_2/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/shortcut/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/shortcut/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/shortcut/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/shortcut/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_1/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block2/unit_2/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/shortcut/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/shortcut/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/shortcut/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/shortcut/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv1/biases_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_1/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block3/unit_2/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/shortcut/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/shortcut/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/shortcut/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/shortcut/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_1/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/preact/beta:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/preact/beta_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv1/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv1/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv2/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv2/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv3/weights:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv3/biases:0 is illegal; using resnet_base/resnet_v2_26/block4/unit_2/bottleneck_v2/conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name resnet_base/resnet_v2_26/postnorm/beta:0 is illegal; using resnet_base/resnet_v2_26/postnorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name class_head/Conv/weights:0 is illegal; using class_head/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name class_head/Conv/biases:0 is illegal; using class_head/Conv/biases_0 instead.\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    training_dataset = tf.data.Dataset().from_generator(local_data_generator, output_types= (tf.float32, tf.int32),\n",
    "                                              output_shapes = (tf.TensorShape([None, IMAGE_HEIGHT, None,1]), \n",
    "                                                                          (tf.TensorShape([None, None]))))\n",
    "    training_dataset = training_dataset.prefetch(BATCH_SIZE)\n",
    "    \n",
    "    string_data_tupels_ph = tf.placeholder(tf.string, shape=[None, None])\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices(string_data_tupels_ph).repeat(-1)\n",
    "\n",
    "    validation_dataset = validation_dataset.map(tf_py_map_func_wrapper, num_parallel_calls=VAL_BATCH_SIZE).prefetch(VAL_BATCH_SIZE)\n",
    "    validation_dataset = validation_dataset.padded_batch(VAL_BATCH_SIZE, padded_shapes=([IMAGE_HEIGHT, None,1],[None]))\n",
    "    validation_dataset = validation_dataset.prefetch(VAL_BATCH_SIZE)\n",
    "    \n",
    "    handle = tf.placeholder(tf.string, shape=[], name='iterator_handler')\n",
    "    iterator = tf.data.Iterator.from_string_handle(handle, training_dataset.output_types, training_dataset.output_shapes)\n",
    "    features, labels = iterator.get_next()\n",
    "    labels_sparce = tf.contrib.layers.dense_to_sparse(labels)\n",
    "    \n",
    "    training_iterator = training_dataset.make_initializable_iterator()    \n",
    "    validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "    \n",
    "    tf.train.create_global_step()\n",
    "\n",
    "    is_training = tf.placeholder_with_default(1, shape=[])\n",
    "    lr = tf.placeholder(shape=[], dtype=tf.float32)\n",
    "    \n",
    "    model = OCRModel(charset=all_chars, input_image_batch=features, \n",
    "                     sequence_labels=labels_sparce, is_training=tf.cast(is_training, tf.bool), learning_rate=lr,) #lr lowered\n",
    "    init = tf.global_variables_initializer()\n",
    "    table_init = tf.tables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    hists = [tf.summary.histogram(name=v.name, values=v.value()) for v in variables]\n",
    "    merged_summary_hists=tf.summary.merge(hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log_lr_decay/model_lev_dist_0.29868379234514775.ckpt\n",
      "0.2984926256406828\n",
      "0.34093216476798727\n",
      "0.3466330272230984\n",
      "0.34026191190906463\n",
      "0.3656119054819974\n",
      "0.3701775419041877\n",
      "0.34799682613392624\n",
      "0.36663306298124704\n",
      "0.34872472063961857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2f6c1f002d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         _, labels_value = sess.run([model.train_op, labels], {handle: training_handle,\n\u001b[0;32m---> 25\u001b[0;31m                                         lr: LR})\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             _, ms = sess.run([model.train_op, model.merged_summary_metrics],\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from tensorflow.python.client import timeline\n",
    "\n",
    "LOG_DIR = 'log_lr_decay'\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR,'train'), sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "    pred_lev_dist = np.Inf\n",
    "                \n",
    "    sess.run([init, training_iterator.initializer, validation_iterator.initializer, table_init],\n",
    "             {string_data_tupels_ph: string_data_tupels})\n",
    "    \n",
    "    training_handle = sess.run(training_iterator.string_handle())\n",
    "    validation_handle = sess.run(validation_iterator.string_handle())\n",
    "    \n",
    "    pred_lev_dist = np.Inf\n",
    "    count_to_decay = 0\n",
    "    try:\n",
    "        saver.restore(sess, os.path.join(LOG_DIR,\"model_lev_dist_0.29868379234514775.ckpt\"))\n",
    "    except:\n",
    "        print('cant restore')\n",
    "    for num in range(int(1e10)):\n",
    "        \n",
    "        _, labels_value = sess.run([model.train_op, labels], {handle: training_handle,\n",
    "                                        lr: LR})\n",
    "        if num%1000 == 0:\n",
    "            _, ms = sess.run([model.train_op, model.merged_summary_metrics],\n",
    "                                      {handle: training_handle,\n",
    "                                       lr: LR})\n",
    "            train_writer.add_summary(ms, num)\n",
    "            train_writer.flush()          \n",
    "            \n",
    "            ms, ms_img = sess.run([model.merged_summary_metrics, model.merged_summary_image], \n",
    "                                   {handle: validation_handle,\n",
    "                                    lr: LR})\n",
    "            test_writer.add_summary(ms, num)\n",
    "            test_writer.add_summary(ms_img, num)\n",
    "            test_writer.flush()\n",
    "            \n",
    "            lev_dist = []\n",
    "            for _ in range(int(VAL_SIZE/VAL_BATCH_SIZE)):\n",
    "                pred_strings, gt_labels = sess.run([model.prediction_string, labels], {handle: validation_handle,\n",
    "                                                                                      is_training: 0, \n",
    "                                                                                      lr: LR})\n",
    "                pred_strings = [pr.decode('utf-8').replace('blank','').upper() for pr in pred_strings]\n",
    "                gt_string = [label_to_string(gl[gl!=0]) for gl in list(gt_labels)]\n",
    "                lev_dist.append(np.mean([distance(ps, gs) for ps, gs in zip(pred_strings, gt_string)]))\n",
    "            lev_dist = np.mean(lev_dist)\n",
    "            print(lev_dist)\n",
    "            if lev_dist < pred_lev_dist:\n",
    "                pred_lev_dist = lev_dist\n",
    "                saver.save(sess, os.path.join(LOG_DIR, \"model_lev_dist_{}.ckpt\".format(lev_dist))) \n",
    "                count_to_decay = 0\n",
    "            else:\n",
    "                count_to_decay += 1\n",
    "            if count_to_decay>LR_DECAY_TOLERANCE:\n",
    "                print('Lowering LR')\n",
    "                count_to_decay = 0\n",
    "                LR /= 2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
