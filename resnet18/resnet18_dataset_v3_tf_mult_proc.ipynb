{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os                                                                          \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"                                       \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  \n",
    "from tensorflow.contrib.slim.nets import resnet_v2, resnet_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers as layers_lib\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.nn import ctc_loss, conv2d\n",
    "import numpy as np\n",
    "resnet_v2_block = resnet_v2.resnet_v2_block\n",
    "resnet_v2 = resnet_v2.resnet_v2\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "RESNET_STRIDE = 1\n",
    "IMAGE_HEIGHT = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v2_26_base(inputs,\n",
    "                 num_classes=None,\n",
    "                 is_training=True, # True - due to update batchnorm layers\n",
    "                 global_pool=False,\n",
    "                 output_stride=1, # effective stride \n",
    "                 reuse=None,\n",
    "                 include_root_block=False, #first conv layer. Removed due to max pool supression. We need large receprive field\n",
    "                 scope='resnet_v2_26'):\n",
    "  \n",
    "    \"\"\"\n",
    "    Tensorflow resnet_v2 use only bottleneck blocks (consist of 3 layers).\n",
    "    Thus, this resnet layer model consist of 26 layers.\n",
    "    I put stride = 2 on each block due to increase receptive field.\n",
    "\n",
    "    \"\"\"\n",
    "    blocks = [\n",
    "      resnet_v2_block('block1', base_depth=64, num_units=2, stride=2),\n",
    "      resnet_v2_block('block2', base_depth=128, num_units=2, stride=2),\n",
    "      resnet_v2_block('block3', base_depth=256, num_units=2, stride=2),\n",
    "      resnet_v2_block('block4', base_depth=512, num_units=2, stride=2),\n",
    "    ]\n",
    "    return resnet_v2(\n",
    "      inputs,\n",
    "      blocks,\n",
    "      num_classes,\n",
    "      is_training,\n",
    "      global_pool,\n",
    "      output_stride,\n",
    "      include_root_block,\n",
    "      reuse=reuse,\n",
    "      scope=scope)\n",
    "\n",
    "def make_ocr_net(inputs, num_classes, is_training=False):\n",
    "    '''\n",
    "    Creates neural network graph.\n",
    "    Image width halved and it's define timestamps width (feature sequence length) \n",
    "    No activation after output (no softmax), due to it's presence at ctc_loss() and beam_search().\n",
    "    After resnet head features are resized to be [batch,1,width,channel], and after that goes 1x1 conv \n",
    "    to make anology of dense connaction for each timestamp.\n",
    "    \n",
    "    input: batch of images\n",
    "    output: tensor of size [batch, time_stamps_width, num_classes]\n",
    "    '''\n",
    "    with tf.variable_scope('resnet_base', values=[inputs]) as sc:\n",
    "        with slim.arg_scope([slim.conv2d],\n",
    "                              activation_fn=None, normalizer_fn=None):\n",
    "            net = resnet_utils.conv2d_same(inputs, 64, 7, stride=2, scope='conv1') #root conv for resnet\n",
    "            #net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1') # due to enlarge of receptive field\n",
    "            net = resnet_v2_26_base(net, output_stride=1, is_training = is_training)[0] # ouput is a tuple of last tensor and all tensors \n",
    "    with tf.variable_scope('class_head', values=[net]) as sc:\n",
    "        net = tf.transpose(net, [0,3,1,2]) # next 4 lines due to column to channel reshape. [batch,c,h,w]\n",
    "        _,c,h,_ = net.get_shape() # depth of input to conv op tensor should be static (defined)\n",
    "        shape = tf.shape(net)\n",
    "        net = tf.reshape(net, [shape[0], c*h, 1, shape[3]])\n",
    "        net = tf.transpose(net,[0,2,3,1]) # back to [batch,h,w,c] = [batch,1,w,features*h]\n",
    "        net = layers_lib.conv2d(net, num_classes, [1, 1], activation_fn=None) #CTC got softmax [batch,1,w,num_classes]\n",
    "        net = tf.squeeze(net,1) #[batch,w,num_classes]\n",
    "        return net\n",
    "\n",
    "def ctc_loss_layer(sequence_labels, logits, sequence_length):\n",
    "    \"\"\"\n",
    "    Build CTC Loss layer for training\n",
    "    sequence_length is a list of siquences lengths, len(sequence_length) = batch_size.\n",
    "    In our case sequences can not be different size due to it origin of images batch, \n",
    "    which should be of equal size (e.g. padded)\n",
    "    \"\"\"\n",
    "    loss = tf.nn.ctc_loss( sequence_labels, \n",
    "                           logits, \n",
    "                           sequence_length,\n",
    "                           time_major=False,  # [batch_size, max_time, num_classes] for logits\n",
    "                           ignore_longer_outputs_than_inputs=True )\n",
    "    total_loss = tf.reduce_mean( loss )\n",
    "    return total_loss\n",
    "\n",
    "def get_training(sequence_labels, net_logits, sequence_length, \n",
    "                   learning_rate=1e-4, decay_steps=2**16, decay_rate=0.9, decay_staircase=False, \n",
    "                   momentum=0.9):\n",
    "    \"\"\"\n",
    "    Set up training ops\n",
    "    https://github.com/weinman/cnn_lstm_ctc_ocr/blob/master/src/model_fn.py\n",
    "    \"\"\"\n",
    "    with tf.name_scope( \"train\" ):\n",
    "        net_logits_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)        \n",
    "        loss = ctc_loss_layer(sequence_labels, net_logits, sequence_length) \n",
    "        # Update batch norm stats [http://stackoverflow.com/questions/43234667]\n",
    "        extra_update_ops = tf.get_collection( tf.GraphKeys.UPDATE_OPS )\n",
    "        with tf.control_dependencies( extra_update_ops ):\n",
    "            # Calculate the learning rate given the parameters\n",
    "            learning_rate_tensor = tf.train.exponential_decay(\n",
    "                learning_rate,\n",
    "                tf.train.get_global_step(),\n",
    "                decay_steps,\n",
    "                decay_rate,\n",
    "                staircase=decay_staircase,\n",
    "                name='learning_rate' )\n",
    "            optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate_tensor,\n",
    "                beta1=momentum )\n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step(),\n",
    "                learning_rate=learning_rate_tensor, \n",
    "                optimizer=optimizer,\n",
    "                variables=net_logits_vars)\n",
    "            tf.summary.scalar('learning_rate', learning_rate_tensor )\n",
    "    return train_op, loss, learning_rate_tensor\n",
    "\n",
    "def get_prediction(output_net, seq_len, merge_repeated=False):\n",
    "    '''\n",
    "    predict by using beam search\n",
    "    input: output_net - logits (without softmax) of net\n",
    "           seq_len - length of predicted sequence \n",
    "    '''\n",
    "    net = tf.transpose(output_net, [1, 0, 2]) #transpose to [time, batch, logits]\n",
    "    decoded, prob = tf.nn.ctc_beam_search_decoder(net, seq_len, merge_repeated=merge_repeated)\n",
    "    return decoded, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRModel(object):\n",
    "    def __init__(self, num_classes, input_image_batch, sequence_labels, is_training=True, learning_rate=1e-4,\n",
    "                decay_steps=2**16, decay_rate=0.9, decay_staircase=False, momentum=0.9):\n",
    "        self.num_classes = num_classes\n",
    "        self.is_training = is_training\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate \n",
    "        self.decay_staircase = decay_staircase\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.build(input_image_batch, sequence_labels)\n",
    "        \n",
    "    def build(self, input_image_batch, sequence_labels):\n",
    "        \n",
    "        self.input_image_batch = input_image_batch\n",
    "        self.sequence_labels = sequence_labels\n",
    "        self.feature_seq_length = tf.fill([tf.shape(self.input_image_batch)[0]], tf.shape(self.input_image_batch)[2]//(2*RESNET_STRIDE)) #as we know effective stride\n",
    "        \n",
    "        net = make_ocr_net(self.input_image_batch, self.num_classes, is_training=self.is_training)\n",
    "        self.net = net\n",
    "        self.train_op, self.loss, self.learning_rate_tensor = get_training(self.sequence_labels, net, self.feature_seq_length,\n",
    "                                                self.learning_rate, self.decay_steps, \n",
    "                                                self.decay_rate, self.decay_staircase, self.momentum)\n",
    "        self.prediction = get_prediction(net, self.feature_seq_length, merge_repeated=False) # tuple(decoded, prob). decoded - list of top paths. I use top1\n",
    "        self.lev_dist = tf.reduce_mean(tf.edit_distance(tf.cast(self.prediction[0][0], tf.int32), self.sequence_labels))\n",
    "        tf.summary.scalar('CTC loss', self.loss)\n",
    "        tf.summary.scalar('Levenshtein distance', self.lev_dist)\n",
    "        tf.summary.scalar('Learning rate', self.learning_rate_tensor)\n",
    "        self.merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../data_generator/')\n",
    "from data_generator import data_generator\n",
    "dg = data_generator(backgrounds_path='../../data_generator/backgrounds/', \n",
    "                                   fonts_path='../../data_generator/valid_fonts/',\n",
    "                                   valid_charset_path='../../data_generator/valid_charset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_data_generator(*args):\n",
    "    image, label = dg.get_image_and_label()\n",
    "    image = image/255\n",
    "    image = image - 0.5\n",
    "    return image.astype(np.float32), label.astype(np.int32)\n",
    "def set_shape_func(image, label):\n",
    "    image.set_shape([IMAGE_HEIGHT, None, 1])\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_chars = ' !\"%&()*+,-./0123456789:=ABCDEFGHIJKLMNOPQRSTUVWXYZ\\\\_abcdefghijklmnopqrstuvwxyz|~ЁАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№'\n",
    "all_chars = list(set(dg.valid_charset))\n",
    "char_to_indx = dict(zip(all_chars,range(len(all_chars))))\n",
    "num_classes = len(all_chars)\n",
    "# def string_to_label(string):\n",
    "#     label = [char_to_indx[s] for s in string]\n",
    "#     return label\n",
    "\n",
    "# def batch_to_sparse(batch, dtype=np.int32): #batch of words\n",
    "#     '''\n",
    "#     function return sparce represantance of labels.\n",
    "#     input: batch - batch of words (List of words)\n",
    "#     output: indices - list of indexes [batch_num,time_stamp_num]\n",
    "#             values - list of char indexes shape [batch]\n",
    "#             shape - shape of dense batch represantation\n",
    "#     '''\n",
    "#     assert isinstance(batch, list) or isinstance(batch, np.ndarray), 'batch should be a list or numpy array of strings'\n",
    "#     indices = [] #[batch_num,w]\n",
    "#     values = [] # char indx\n",
    "#     for batch_num, word in enumerate(batch):\n",
    "#         assert isinstance(word,str), 'batch element should be a string'\n",
    "#         word_as_indx = string_to_label(word)\n",
    "#         indices.extend([(batch_num,char_num) for char_num, char in enumerate(word_as_indx)])\n",
    "#         values.extend([char for char_num, char in enumerate(word_as_indx)])\n",
    "#     indices = np.asarray(indices, dtype=dtype)\n",
    "#     values = np.asarray(values, dtype=dtype)\n",
    "#     shape = np.array([len(batch),indices.max(0)[1]+1], dtype=dtype)\n",
    "#     return indices, values, shape\n",
    "# def from_sparse_to_batch():\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name CTC loss is illegal; using CTC_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name CTC loss is illegal; using CTC_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Levenshtein distance is illegal; using Levenshtein_distance instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Levenshtein distance is illegal; using Levenshtein_distance instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Learning rate is illegal; using Learning_rate instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Learning rate is illegal; using Learning_rate instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "#     dataset = tf.data.Dataset().from_generator(dg.get_image_and_label, output_types= (tf.float32, (tf.int64,tf.int32,tf.int64)),\n",
    "#                                               output_shapes = (tf.TensorShape([None,32,None,1]), \n",
    "#                                                                           (tf.TensorShape([None,None]),\n",
    "#                                                                            tf.TensorShape([None]),\n",
    "#                                                                            tf.TensorShape([None]))))\n",
    "    dataset = tf.data.Dataset().range(int(1e8))\n",
    "#     dataset = dataset.map(local_data_generator, num_parallel_calls=4).prefetch(32)\n",
    "#     dataset = dataset.map(set_shape_func, num_parallel_calls=4).prefetch(32)\n",
    "    dataset = dataset.interleave(lambda x: tf.data.Dataset.from_tensors(local_data_generator()), cycle_length=32, block_length=32)\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(32)\n",
    "    data_iter = dataset.make_initializable_iterator()\n",
    "    features, labels = data_iter.get_next()\n",
    "    labels = tf.contrib.layers.dense_to_sparse(labels)\n",
    "    tf.train.create_global_step()\n",
    "    model = OCRModel(num_classes=num_classes+2, input_image_batch=features, \n",
    "                     sequence_labels=labels, is_training=True)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-008c2f81e377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged_summary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "LOG_DIR = 'log_temp_temp/'\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + 'train',\n",
    "                                          sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(LOG_DIR + 'test')\n",
    "    sess.run([init, data_iter.initializer])\n",
    "#     try:\n",
    "#         saver.restore(sess, \"log/model.ckpt\")\n",
    "#     except:\n",
    "#         print('cant restore')\n",
    "    for num in range(int(10)):\n",
    "        print(num)\n",
    "        _, ms = sess.run([model.train_op, model.merged_summary])\n",
    "        train_writer.add_summary(ms, num)\n",
    "        if num%100 == 0:\n",
    "            _, ms_ = sess.run([model.train_op, model.merged_summary])\n",
    "            test_writer.add_summary(ms_, num)\n",
    "            saver.save(sess, LOG_DIR + \"model.ckpt\")\n",
    "print(time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
